import sys
import ipaddress
import os
import json
import subprocess
from pathlib import Path
import httpx
import time
from urllib.parse import urlparse
from collections import defaultdict
from tld import get_fld
from rapiddns import RapidDns
from tqdm import tqdm
import asyncio
import socket
from concurrent.futures import ThreadPoolExecutor
import multiprocessing
from itertools import islice
from functools import partial
import shutil
import tldextract
import re
import signal
import requests
from datetime import datetime
import random
import base64
import configparser
import aiofiles

# ------------------------------------
# å‘½ä»¤æ¨¡æ¿å’Œé…ç½®
# é¦–å…ˆè·å–é¡¹ç›®æ ¹ç›®å½•
# ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç›¸å¯¹è·¯å¾„æ¨å¯¼
if 'SCAN_PROJECT_ROOT' in os.environ:
    PROJECT_ROOT = os.environ['SCAN_PROJECT_ROOT']
else:
    # Fallback: ä»è„šæœ¬ä½ç½®æ¨å¯¼é¡¹ç›®æ ¹ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    PROJECT_ROOT = os.path.abspath(os.path.join(script_dir, '../..'))

# PROJECT_ROOT åˆå§‹åŒ–å®Œæˆ
# è·å–å·¥å…·è·¯å¾„
TOOLS_PATH = os.path.join(PROJECT_ROOT, "tools/scanner")

if '-small' in sys.argv or '-test' in sys.argv:
    print("[*] ä½¿ç”¨æµ‹è¯•ç¯å¢ƒå‘½ä»¤æ¨¡æ¿")
    AFROG_CMD_TEMPLATE = f"{TOOLS_PATH}/afrog -T {{target_file}} -c 100 -rl 300 -timeout 2 -s spring -doh -json {{output_file}}"
    FSCAN_CMD_TEMPLATE = f"{TOOLS_PATH}/fscan -hf {{target_file}} -p 80 -np -nobr -t 600 -o {{output_file}}"
    DEBUG_FSCAN = True
else:
    print("[*] ä½¿ç”¨æ­£å¼ç¯å¢ƒå‘½ä»¤æ¨¡æ¿")
    AFROG_CMD_TEMPLATE = f"{TOOLS_PATH}/afrog -T {{target_file}} -c 100 -rl 300 -timeout 2 -S high,info -doh -json {{output_file}}"
    FSCAN_CMD_TEMPLATE = f"{TOOLS_PATH}/fscan -hf {{target_file}} -p all -np -nobr -t 600  -o {{output_file}}"
    DEBUG_FSCAN = True
ONLY_DOMAIN_MODE = '-test' in sys.argv
RESULT_JSON_PATH = "temp/result_all.json"

if ONLY_DOMAIN_MODE:

    print("[*] ä»…å¤„ç†åŸŸåæ¨¡å¼ (-test)ï¼Œå°†è·³è¿‡å®‰å…¨æ‰«æä»»åŠ¡")
SKIP_CURRENT_DOMAIN = False

# ä½¿ç”¨ç¯å¢ƒå˜é‡è·å–é…ç½®æ–‡ä»¶è·¯å¾„
CDN_LIST_PATH = os.path.join(PROJECT_ROOT, "config/filters/cdn.txt")
CDN_DYNAMIC_PATH = os.path.join(PROJECT_ROOT, "config/filters/cdn_åŠ¨æ€æ·»åŠ _ä¸€å¹´æ¸…ä¸€æ¬¡.txt")
DYNAMIC_FILTER_FILE = Path(os.path.join(PROJECT_ROOT, "config/filters/filter_domains-åŠ¨æ€.txt"))
new_filtered_domains = set()

black_titles = {
        "Just a moment...",
        "Attention Required! | Cloudflare",
        "å®‰å…¨éªŒè¯",  # å¯æ ¹æ®ä½ ä¸šåŠ¡æ·»åŠ æ›´å¤šæ— æ•ˆæ ‡é¢˜
}
# 1. è¯»å–å·²æœ‰çš„åŠ¨æ€è¿‡æ»¤åŸŸå
# âœ… åŒæ­¥è¯»å–æ–¹å¼ï¼Œæœ€ç®€å•ç¨³å®šï¼ˆæ¨èç”¨äºéasyncç¨‹åºï¼‰
if DYNAMIC_FILTER_FILE.exists():
    with open(DYNAMIC_FILTER_FILE, mode='r', encoding='utf-8') as f:
        for line in f:
            line = line.strip().strip('"').strip("'").lower()
            if line:
                new_filtered_domains.add(line)


#è¿‡æ»¤
FILTER_DOMAIN_PATH = os.path.join(PROJECT_ROOT, "config/filters/filter-domain.txt")
BLACKLIST_FILE_PATH = os.path.join(PROJECT_ROOT, "config/filters/fofa_query_blacklist.txt")


hunter_proxies = "socks5h://127.0.0.1:7891"
config_path = Path(os.path.join(PROJECT_ROOT, "config/api/config.ini"))
config = configparser.ConfigParser()
config.read(config_path, encoding='utf-8')

TEST_EMAIL = config['DEFAULT'].get('TEST_EMAIL')
TEST_KEY = config['DEFAULT'].get('TEST_KEY')
HUNTER_API_KEY = ""

dns_cache = {}
reverse_lookup_semaphore = None  # å°†åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­åˆå§‹åŒ–

def handle_sigint(signum, frame):
    global SKIP_CURRENT_DOMAIN
    print("\n[!] æ”¶åˆ° Ctrl+Cï¼Œè·³è¿‡å½“å‰åŸŸåï¼Œç»§ç»­ä¸‹ä¸€ä¸ª...")
    SKIP_CURRENT_DOMAIN = True
def headers_lib():
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36"
    }
def handle_sigquit(signum, frame):
    print("\n[!] æ”¶åˆ° Ctrl+\\ï¼Œç»ˆæ­¢æ•´ä¸ªç¨‹åº")
    sys.exit(0)
def is_domain_resolvable(domain):
    if domain in dns_cache:
        return dns_cache[domain]
    try:
        socket.gethostbyname(domain)
        dns_cache[domain] = True
        return True
    except Exception:
        dns_cache[domain] = False
        return False
# ------------------------------------
async def reverse_lookup_ip_async(ip):
    print("[>] ä½¿ç”¨ dnsdblookup åæŸ¥åŸŸåæ¥å£")
    try:
        url_d = f"https://dnsdblookup.com/{ip}/"
        async with httpx.AsyncClient(timeout=5) as client:
            res = await client.get(url_d, headers=headers_lib())
        site = re.findall(r'<span class="date">(.*?)</span><a href="/(.*?)/" target="_blank">(.*?)</a>', res.text, re.S)

        domains = [domain for _, _, domain in site]

        # å»é‡ï¼Œé¿å…é‡å¤åŸŸåå½±å“åç»­é€»è¾‘
        domains = list(set(domains))

        if domains:
            return ip, domains
        else:
            return ip, []

    except Exception as e:
        print(f"[!] dnsdblookup åæŸ¥å¤±è´¥: {e}")
        try:
            print("[>] ä½¿ç”¨ RapidDns åæŸ¥åŸŸåæ¥å£")
            domains = RapidDns.sameip(ip)
            # æ ¼å¼ç»Ÿä¸€ä¸ºæ‰å¹³åŒ–å­—ç¬¦ä¸²åˆ—è¡¨
            flat_domains = [item[0] if isinstance(item, list) else item for item in domains]
            return ip, list(set(flat_domains))

        except Exception as e:
            print(f"[!] RapidDns åæŸ¥å¤±è´¥: {ip}, é”™è¯¯: {e}")
            print("[>] ä½¿ç”¨ ip138 åæŸ¥åŸŸåæ¥å£")
            try:
                url_d_138 = f"https://ip138.com/{ip}/"
                async with httpx.AsyncClient(timeout=5) as client:
                    res_138 = await client.get(url_d_138, headers=headers_lib())
                site_138 = re.findall(r'<span class="date">(.*?)</span><a href="/(.*?)/" target="_blank">(.*?)</a>', res_138.text, re.S)

                # ä¸åšæ—¶é—´è¿‡æ»¤ï¼Œç›´æ¥å…¨éƒ¨åŸŸå
                domains = [domain_138 for _, _, domain_138 in site_138]
                domains = list(set(domains))

                if domains:
                    return ip, domains
                else:
                    return ip, []
            except Exception as e:
                print(f"[!] ip138 åæŸ¥å¤±è´¥: {e}")
                return ip, []


        return ip, []

    return ip, None

# å¼‚æ­¥æ‰§è¡Œå‘½ä»¤
async def run_cmd_async(cmd):
    if DEBUG_FSCAN:
        print(f"[cmd] å¼‚æ­¥æ‰§è¡Œå‘½ä»¤: {cmd}")
    proc = await asyncio.create_subprocess_shell(
        cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await proc.communicate()
    stdout_str = stdout.decode(errors='ignore').strip()
    stderr_str = stderr.decode(errors='ignore').strip()

    if proc.returncode != 0:
        print(f"[ERROR] å‘½ä»¤æ‰§è¡Œå¤±è´¥: {cmd}")
        print(f"[ERROR] è¿”å›ç : {proc.returncode}")
        print(f"[ERROR] stderr: {stderr_str}")
        return None, stderr_str  # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯é€€å‡º

    # await finalize_report_directory(report_path, root)

    return stdout_str, stderr_str
# ------------------------------------
# ç›®å½•åˆå§‹åŒ–
def init_dirs():
    for d in ["temp", "output"]:
        os.makedirs(d, exist_ok=True)

# è½½å…¥è¿‡æ»¤åŸŸå
def load_filter_domains(path):
    if os.path.exists(path):
        return {line.strip().lower() for line in open(path, encoding="utf-8") if line.strip()}
    return set()

# è½½å…¥CDN IPæ®µ
def load_cdn_ranges(path):
    ranges = []
    if os.path.exists(path):
        for line in open(path, encoding="utf-8"):
            line = line.strip()
            if line:
                try:
                    net = ipaddress.ip_network(line if '/' in line else line + '/32', strict=False)
                    ranges.append(net)
                except ValueError:
                    print(f"[!] æ— æ•ˆCDNæ¡ç›®: {line}")
    return ranges

# åˆ¤æ–­IPæ˜¯å¦å±äºCDN
def is_cdn_ip(ip, cdn_ranges):
    try:
        ip_obj = ipaddress.ip_address(ip)
        return any(ip_obj in net for net in cdn_ranges)
    except ValueError:
        return False

# ------------------------------------
# å¤šè¿›ç¨‹è§£æJSONå—ï¼Œå¢åŠ   ä¿¡æ¯æ”¶é›†
def parse_json_lines_chunk(lines_chunk, cdn_ranges, existing_cdn_dyn_ips, filter_domains):
    domain_ip_map = defaultdict(set)
    url_title_list = []
    url_root_map = {}
    url_body_info_map = {}
    filtered_non_200_urls = []  # æ–°å¢ï¼Œç”¨äºä¿å­˜é200/301/302çš„urlå’ŒçŠ¶æ€ç 
    redirect_domains_set = set()  # æ–°å¢ï¼Œç”¨äºä¿å­˜è·³è½¬å‘ç°çš„åŸŸå
    body_fqdn_filtered_set = set()
    body_domains_filtered_set = set()
    # ä½¿ç”¨ç¯å¢ƒå˜é‡è·å–tlds.txtè·¯å¾„
    tlds_path = os.path.join(PROJECT_ROOT, "config/tlds.txt")
    
    tlds_content = None
    try:
        with open(tlds_path, "r", encoding="utf-8") as f:
            tlds_content = f.read()
    except FileNotFoundError:
        print(f"[!] è­¦å‘Š: æ— æ³•æ‰¾åˆ°{tlds_path}æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤TLDåˆ—è¡¨")
        tlds_content = "com\nnet\norg\nedu\ngov\nmil\ninfo\nbiz\nname\ncn\nuk\nde\nfr\njp\nkr\nau\nca\nru\nbr\nin\nit\nes\nnl\nse\nno\ndk\nfi\npl\nbe\nch\nat\ncz\nhu\npt\ngr\ntr\nil\nza\nmx\nsg\nhk\ntw\nmy\nth\nph\nvn\nid\n"
    
    VALID_TLDS = set(line.strip().lower() for line in tlds_content.strip().split('\n') if line.strip())
    seen_ips = set()
    for idx, line in enumerate(lines_chunk):
        try:
            item = json.loads(line)
            url = item.get("url", "").strip()
            final_url = item.get("final_url", "").strip()  # ä½¿ç”¨-follow-redirectsæ—¶çš„æœ€ç»ˆURL
            location_url = item.get("location", "").strip()  # ä¸ä½¿ç”¨-follow-redirectsæ—¶çš„è·³è½¬ä½ç½®
            
            # å¤„ç†è·³è½¬ä¿¡æ¯ï¼ˆæ”¯æŒä¸¤ç§æƒ…å†µï¼‰
            redirect_url = final_url if (final_url and final_url != url) else location_url
            
            # å¦‚æœå­˜åœ¨è·³è½¬ï¼Œè®°å½•è·³è½¬ä¿¡æ¯ç”¨äºåç»­èµ„äº§å‘ç°
            if redirect_url:
                try:
                    redirect_parsed = urlparse(redirect_url)
                    if redirect_parsed.hostname:
                        redirect_hostname = redirect_parsed.hostname.lower()
                        # æå–è·³è½¬åŸŸåçš„æ ¹åŸŸå
                        try:
                            redirect_root = get_fld(redirect_url, fix_protocol=False).lower()
                            # é¿å…è®°å½•ç›¸åŒçš„æ ¹åŸŸå
                            original_root = get_fld(url, fix_protocol=False).lower()
                            if redirect_root != original_root:
                                redirect_domains_set.add(redirect_root)
                                if DEBUG_FSCAN:
                                    print(f"[+] å‘ç°è·³è½¬åŸŸå: {url} -> {redirect_url} (æ–°åŸŸå: {redirect_root})")
                        except Exception:
                            # å¦‚æœæ— æ³•æå–æ ¹åŸŸåï¼Œç›´æ¥ä½¿ç”¨hostname
                            redirect_domains_set.add(redirect_hostname)
                except Exception:
                    pass

            title = item.get("title", "").strip()
            tls_info = item.get("tls", {})  
            cert = tls_info.get("subject_cn", "").strip()
            ico = item.get("favicon_md5", "").strip()
            ico_mmh3 = item.get("favicon", "").strip()
            hash_info = item.get("hash", {})
            bd_hash = hash_info.get("body_md5", "").strip()
            bd_mmh3 = hash_info.get("body_mmh3", "").strip()
            a_ips = item.get("a", [])

            try:
                parsed_url = urlparse(url)
                hostname = parsed_url.hostname
                # åˆ¤æ–­æ˜¯å¦æ˜¯IP
                ipaddress.ip_address(hostname)
                root_domain = hostname  # ç›´æ¥ç”¨ IP
            except ValueError:
                try:
                    root_domain = get_fld(url, fix_protocol=False).lower()
                except Exception as e:
                    if DEBUG_FSCAN:
                        print(f"[!] æå–ä¸»åŸŸåå¤±è´¥: {url} é”™è¯¯: {e}")
                    continue
            url_root_map[url] = root_domain
            status_code = item.get("status_code")  # ç¡®è®¤å®é™…å­—æ®µ
            if status_code is None:
                status_code = 0  # æˆ–è€…é»˜è®¤ä¸€ä¸ªå€¼ï¼Œé˜²æ­¢æŠ¥é”™
            # ç‰¹æ®ŠçŠ¶æ€ç å•ç‹¬å¤„ç†
            if status_code in (403, 404):
                filtered_non_200_urls.append((url, status_code))
                continue  # è·³è¿‡æ­£å¸¸æµç¨‹ï¼Œä½†è®°å½•ç‰¹æ®ŠçŠ¶æ€ç 
            elif status_code not in (200, 301, 302):
                # å…¶ä»–éæ­£å¸¸çŠ¶æ€ç ä¹Ÿè®°å½•
                filtered_non_200_urls.append((url, status_code))
                continue  # è·³è¿‡åç»­æ­£å¸¸æµç¨‹
            url_title_list.append((url, title, cert, ico, bd_hash, tuple(sorted(a_ips)),ico_mmh3,bd_mmh3))

            for ip in a_ips:
                if is_cdn_ip(ip, cdn_ranges):
                    continue
                if ip in existing_cdn_dyn_ips:
                    continue
                if ip in seen_ips:
                    continue
                seen_ips.add(ip)
                domain_ip_map[root_domain].add(ip)
            body_fqdn_list = item.get("body_fqdn", [])
            body_domains_list = item.get("body_domains", [])

            filtered_fqdn = []
            for fqdn in body_fqdn_list:
                if fqdn  and "cdn" not in fqdn and "img" not in fqdn:
                    try:
                        ext = tldextract.extract(fqdn)
                        if ext.domain and ext.suffix and ext.suffix.lower() in VALID_TLDS:
                            root_domain = f"{ext.domain}.{ext.suffix}".lower()
                            if root_domain not in filter_domains and root_domain not in new_filtered_domains:
                                if is_domain_resolvable(root_domain):
                                    filtered_fqdn.append(fqdn.lower())
                                    new_filtered_domains.add(root_domain)

                    except Exception:
                        pass

            filtered_domains = []
            for domain in body_domains_list:
                if domain  and "cdn" not in domain and "img" not in domain:
                    try:
                        ext = tldextract.extract(domain)
                        if ext.domain and ext.suffix and ext.suffix.lower() in VALID_TLDS:
                            root_domain = f"{ext.domain}.{ext.suffix}".lower()
                            if root_domain not in filter_domains and root_domain not in new_filtered_domains:
                                if is_domain_resolvable(root_domain):
                                    filtered_domains.append(domain.lower())
                                    new_filtered_domains.add(root_domain)

                    except Exception:
                        pass

            # ä¿å­˜ç»“æœ
            url_body_info_map[url] = {
                "body_fqdn": filtered_fqdn,
                "body_domains": filtered_domains
            }
            if new_filtered_domains:
                with open(DYNAMIC_FILTER_FILE, "a", encoding="utf-8") as f:
                    for dom in sorted(new_filtered_domains):
                        f.write(dom + "\n")

        except Exception as e:
            if DEBUG_FSCAN:
                print(f"[!] JSONè§£æå¼‚å¸¸ (ç¬¬ {idx} è¡Œ): {e}")
            continue

    return domain_ip_map, url_title_list, url_root_map, url_body_info_map, filtered_non_200_urls, redirect_domains_set

def chunked_iterable(iterable, size):
    """æŒ‰sizeåˆ‡åˆ†è¿­ä»£å™¨æˆå°å—"""
    it = iter(iterable)
    while True:
        chunk = list(islice(it, size))
        if not chunk:
            break
        yield chunk

# ------------------------------------
# å°è£…ï¼šç¡®ä¿ base_info æ–‡ä»¶å­˜åœ¨ï¼ˆå¦‚æ— åˆ™åæŸ¥å¹¶å†™å…¥ï¼‰
async def ensure_base_info(root, report_path, valid_ips, urls, titles, filter_domains, existing_cdn_dyn_ips, url_body_info_map, folder, redirect_domains=None):
    base_info_files = list(report_path.glob(f"base_info_{root}.txt"))

    if base_info_files:
        print(f"[i] base_info æ–‡ä»¶å­˜åœ¨ï¼Œè·³è¿‡å†™å…¥ base_info")
        return None  # å·²æœ‰æ–‡ä»¶ï¼Œä¸éœ€è¦åæŸ¥
    else:
        print(f"[i] base_info æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹åæŸ¥å¹¶å†™å…¥ base_info")
        ip_domain_map = await resolve_and_filter_domains(valid_ips, filter_domains, existing_cdn_dyn_ips, folder)
        print("[âœ“] å®ŒæˆåæŸ¥åŸŸå")
        print(ip_domain_map)
        await write_base_report(root, report_path, valid_ips, urls, titles, ip_domain_map, url_body_info_map, redirect_domains)
        return ip_domain_map
async def per_domain_flow_sync_async(root, ips, urls, titles, cdn_ranges, filter_domains, existing_cdn_dyn_ips, url_body_info_map, redirect_domains=None):
    print(f"\n[>] æ‰§è¡ŒåŸŸåæµç¨‹: {root}")
    folder = prepare_domain_folder(root)
    valid_ips = write_valid_ips(folder, ips, cdn_ranges, existing_cdn_dyn_ips)
    write_urls(folder, urls)
    mark_classification_complete(folder)

    # æŠ¥å‘Šç›®å½•è®¾ç½®
    base_report_root = Path("output")
    standard_dir = base_report_root / root
    finish_dir = base_report_root / f"{root}_finish"
    exp_dir = base_report_root / f"{root}_vul"

    if finish_dir.exists():
        print(f"[i] å‘ç°å·²æœ‰å®ŒæˆæŠ¥å‘Šç›®å½•: {finish_dir}")
        return  # å·²å®Œæˆï¼Œè·³è¿‡å¤„ç†
    elif exp_dir.exists():
        report_path = exp_dir
        print(f"[i] å‘ç°å·²æœ‰æ¼æ´æŠ¥å‘Šç›®å½•: {report_path}")
    elif standard_dir.exists():
        report_path = standard_dir
        print(f"[i] ä½¿ç”¨å·²æœ‰æ‰«æä¸­ç›®å½•: {report_path}")
    else:
        report_path = standard_dir
        report_path.mkdir(parents=True, exist_ok=True)
        print(f"[+] åˆ›å»ºæ–°æŠ¥å‘Šç›®å½•: {report_path}")

    # è·å–ç›®å½•ä¸‹å·²æœ‰æ–‡ä»¶
    files = list(report_path.iterdir())

    if not files:
        print(f"[+] æŠ¥å‘Šç›®å½•ä¸ºç©ºï¼Œå¼€å§‹æ­£å¸¸æ‰«æ")
        print(f"[*] æœ‰æ•ˆIPåˆ—è¡¨: {valid_ips}")
        print(f"å½“å‰åŸŸå: {root}")

        ip_domain_map,cdn_ip_to_remove = await resolve_and_filter_domains(valid_ips, filter_domains, existing_cdn_dyn_ips, folder)
        print("[âœ“] å®ŒæˆåæŸ¥åŸŸå")
        valid_ips = [ip for ip in valid_ips if ip not in cdn_ip_to_remove]

        await write_base_report(root, report_path, valid_ips, urls, titles, ip_domain_map, url_body_info_map, redirect_domains)
        await write_representative_urls(folder, titles, urls)
        if not ONLY_DOMAIN_MODE:
            await run_security_scans(root, folder, report_path)

    else:
        ip_domain_map = await ensure_base_info(
            root, report_path, valid_ips, urls, titles,
            filter_domains, existing_cdn_dyn_ips, url_body_info_map, folder, redirect_domains
        )

        base_info_files = list(report_path.glob(f"base_info_{root}.txt"))

        has_scan_done = any(f.name == "æ‰«æå®Œæˆ.txt" for f in files)
        if base_info_files and has_scan_done:
            print(f"[âœ“] ç›®æ ‡ {root} å·²å®Œæˆæ‰«æï¼ˆå­˜åœ¨ base_info å’Œ æ‰«æå®Œæˆ.txtï¼‰ï¼Œè·³è¿‡ã€‚")
            return

        elif base_info_files:
            print(f"[+] åªæœ‰ base_info æ–‡ä»¶ï¼Œå‡†å¤‡å¤„ç†")

            # æ— è®ºå¦‚ä½•éƒ½è¦å¤„ç†æ‰©å±•ç»“æœ
            await merge_all_expanded_results(str(report_path), root, redirect_domains)

            if ONLY_DOMAIN_MODE:
                print(f"[i] è·³è¿‡ run_security_scansï¼Œå› å¯ç”¨äº† --test")
                return

            await run_security_scans(root, folder, report_path)


def prepare_domain_folder(root):
    folder = Path("output") / root
    folder.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºå­ç›®å½•ç»“æ„
    input_folder = folder / "input"  # å­˜æ”¾æ‰«æè¾“å…¥æ–‡ä»¶
    input_folder.mkdir(exist_ok=True)
    
    print(f"[âœ“] åˆ›å»ºåŸŸåç›®å½•: {folder}")
    return folder
def natural_sort_key(s):
    # åˆ†å‰²å­—ç¬¦ä¸²ï¼Œæ•°å­—è½¬intï¼Œå­—æ¯å°å†™
    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\d+)', s)]

def write_valid_ips(folder, ips, cdn_ranges, existing_cdn_dyn_ips):
    valid_ips = []
    input_folder = folder / "input"
    input_folder.mkdir(exist_ok=True)

    # å…ˆè¯»å– all_a_records.txtï¼ˆå¦‚æœå­˜åœ¨ï¼‰é‡Œçš„å†å² IP
    all_a_records_path = input_folder / "all_a_records.txt"
    if all_a_records_path.exists():
        with open(all_a_records_path, "r") as f:
            existing_all_ips = set(line.strip() for line in f if line.strip())
    else:
        existing_all_ips = set()

    with open(input_folder / "a_records.txt", "w") as a, open(all_a_records_path, "a") as all_a:
        for ip in sorted(ips):
            if is_cdn_ip(ip, cdn_ranges) or ip in existing_cdn_dyn_ips:
                print(f"[-] CDNè·³è¿‡: {ip}")
                continue
            if ip in existing_all_ips:
                print(f"[!] å·²å­˜åœ¨äº all_a_records.txt ä¸­ï¼Œè·³è¿‡: {ip}")
                continue
            a.write(ip + "\n")
            all_a.write(ip + "\n")
            valid_ips.append(ip)

    return valid_ips


def write_urls(folder, urls):
    input_folder = folder / "input"
    input_folder.mkdir(exist_ok=True)
    with open(input_folder / "urls.txt", "w") as u:
        for url in urls:
            u.write(url + "\n")


def mark_classification_complete(folder):
    try:
        # finish.txt ä¿ç•™åœ¨æ ¹ç›®å½•ä½œä¸ºå®Œæˆæ ‡è®°
        with open(folder / "finish.txt", "w", encoding="utf-8") as f:
            f.write("åˆ†ç±»å®Œæˆ")
        print(f"[âœ“] æ ‡è®°åˆ†ç±»å®Œæˆ: {folder}/finish.txt")
    except Exception as e:
        print(f"[!] å†™å…¥ finish.txt å¤±è´¥: {e}")

def create_simplified_output(root, report_folder):
    """åˆ›å»ºç®€åŒ–çš„è¾“å‡ºç»“æ„ï¼Œåªä¿ç•™æ ¸å¿ƒæ–‡ä»¶"""
    core_folder = Path("output") / root
    core_folder.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ç®€åŒ–ç›®å½•ï¼ˆé¿å…é‡å¤å¤åˆ¶ï¼‰
    if str(report_folder.resolve()) == str(core_folder.resolve()):
        print(f"[i] å·²æ˜¯ç®€åŒ–è¾“å‡ºç›®å½•ï¼Œæ— éœ€å¤åˆ¶")
        return core_folder
    
    # åªå¤åˆ¶æ ¸å¿ƒæ–‡ä»¶åˆ°ç®€åŒ–ç›®å½•
    core_files = [
        f"base_info_{root}.txt",
        "finish.txt"
    ]
    
    for file_name in core_files:
        src_file = report_folder / file_name
        dst_file = core_folder / file_name
        if src_file.exists():
            shutil.copy2(src_file, dst_file)
            print(f"[âœ“] å¤åˆ¶æ ¸å¿ƒæ–‡ä»¶: {file_name}")
    
    # å¤åˆ¶æ‰«å±•æ•°æ®ç›®å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    tuozhan_src = report_folder / "tuozhan"
    tuozhan_dst = core_folder / "tuozhan"
    if tuozhan_src.exists():
        if tuozhan_dst.exists():
            shutil.rmtree(tuozhan_dst)
        shutil.copytree(tuozhan_src, tuozhan_dst)
        print(f"[âœ“] å¤åˆ¶æ‰©å±•æ•°æ®ç›®å½•: tuozhan")
    
    # å¤åˆ¶inputç›®å½•ï¼ˆåŒ…å«æ‰«æè¾“å…¥æ•°æ®å’ŒæŠ¥å‘Šæ–‡ä»¶ï¼‰
    input_src = report_folder / "input"
    input_dst = core_folder / "input"
    if input_src.exists():
        if input_dst.exists():
            shutil.rmtree(input_dst)
        shutil.copytree(input_src, input_dst)
        print(f"[âœ“] å¤åˆ¶è¾“å…¥æ•°æ®ç›®å½•: input")
    
    print(f"[âœ“] åˆ›å»ºç®€åŒ–è¾“å‡º: {core_folder}")
    return core_folder


def create_report_folder(root):
    report_folder = Path("output") / root
    report_folder.mkdir(parents=True, exist_ok=True)
    print(f"[âœ“] åˆ›å»ºæŠ¥å‘Šç›®å½•: {report_folder}")
    return report_folder

def update_a_records_after_scan(cdn_ip_to_remove, a_record_file):
    path = a_record_file / "input" / "a_records.txt"
    if not path.exists():
        print(f"[!] æœªæ‰¾åˆ°æ–‡ä»¶: {a_record_file}/input/a_records.txt")
        return

    with open(path, "r") as f:
        lines = f.readlines()

    new_lines = [line for line in lines if line.strip() not in cdn_ip_to_remove]

    with open(path, "w") as f:
        f.writelines(new_lines)

    print(f"[âœ“] å·²ä» a_records.txt ä¸­ç§»é™¤ {cdn_ip_to_remove} ")


CDN_KEYWORDS = [
    "cloudfront.net", "r.cloudfront.net",
    "cloudflare.com", "cloudflare.net",
    "akamai", "akamaiedge.net", "akamaized.net", "akamaitechnologies.com",
    "fastly.net", "fastlylb.net",
    "googleusercontent.com", ".gws",
    "dnsv1.com", "tcdn.qq.com",
    "baidubce.com",
    "alicdn.com", "aliyun.com",
    "wscdns.com", "wscloudcdn.com",
    "edgecastcdn.net",
    "cdnetworks.net", "cdngc.net",
    "incapdns.net", "impervadns.net"
]

def is_cdn_domain(domain: str) -> bool:
    return any(keyword in domain.lower() for keyword in CDN_KEYWORDS)
def is_cdn_ip_new(ip, domains):
    # print(f"[+] åˆ¤æ–­IP: {ip} æ˜¯å¦æ˜¯CDNèŠ‚ç‚¹")
    
    # æ¡ä»¶1ï¼šåŸŸåæ•°é‡è¿‡å¤šï¼Œç›´æ¥åˆ¤å®šä¸ºCDN
    if len(domains) > 45:
        print(f"[-] åŸŸåæ•°é‡å¤§äº45), ç›´æ¥åˆ¤å®šä¸ºCDN")
        return True

    # éšæœºé€‰ä¸€ä¸ªåŸŸååšæµ‹è¯•
    test_domain = random.choice(domains)
    # print(f"[+] é€‰å–çš„æµ‹è¯•åŸŸå: {test_domain}")

    try:
        # æ­£å‘è§£æï¼šåŸŸå -> IPåˆ—è¡¨
        ips = socket.gethostbyname_ex(test_domain)[2]
        # print(f"[+] æ­£å‘è§£æ {test_domain} å¾—åˆ°IPåˆ—è¡¨: {ips}")
        
        if ip not in ips:
            # print(f"[-] ç›®æ ‡IP {ip} ä¸åœ¨åŸŸåè§£æçš„IPåˆ—è¡¨ä¸­ï¼Œåˆ¤å®šä¸ºCDN")
            return True

        if len(ips) > 4:
            # print(f"[-] æ­£å‘è§£æIPåˆ—è¡¨æ•°é‡è¶…è¿‡4 ({len(ips)}), åˆ¤å®šä¸ºCDN")
            return True

    except Exception as e:
        # print(f"[-] è§£æå¼‚å¸¸: {e}ï¼Œåˆ¤å®šä¸ºCDN")
        return True

    print(f"[+] é€šè¿‡æ‰€æœ‰åˆ¤æ–­ {ip} éCDNèŠ‚ç‚¹")
    return False

async def resolve_and_filter_domains(valid_ips, filter_domains, existing_cdn_dyn_ips, folder):
    global reverse_lookup_semaphore
    if reverse_lookup_semaphore is None:
        reverse_lookup_semaphore = asyncio.Semaphore(3)  # é™åˆ¶å¹¶å‘åæŸ¥æ•°é‡
    
    ip_domain_map = defaultdict(list)
    cdn_ip_to_remove = set()
    
    # ä½¿ç”¨å¼‚æ­¥å¹¶å‘å¤„ç†åæŸ¥
    async def process_ip(ip):
        async with reverse_lookup_semaphore:
            return await reverse_lookup_ip_async(ip)
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰IPåæŸ¥
    tasks = [process_ip(ip) for ip in valid_ips]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"[!] IP {valid_ips[i]} åæŸ¥å¤±è´¥: {result}")
            continue
        
        ip_, domains = result
        if not domains:
            print(f"[!] {ip_} åæŸ¥æ— ç»“æœ")
            continue

        if is_cdn_ip_new(ip_, domains):
            print(f"[!] {ip_} è¯†åˆ«ä¸ºCDN IPï¼Œç§»é™¤")
            cdn_ip_to_remove.add(ip_)
        else:
            ip_domain_map[ip_].extend(domains)

        is_cdn = False
        for d in domains:
            try:
                if isinstance(d, list):  # ä¿®å¤ç‚¹
                    d = d[0]
                domain_line = d.strip()
                match = re.search(r'([a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}', domain_line)
                if not match:
                    continue

                domain = match.group(0)

                if is_cdn_domain(domain):
                    print(f"[!] CDN åŸŸå {domain}ï¼Œæ ‡è®° CDN IP: {ip_}")
                    cdn_ip_to_remove.add(ip_)
                    is_cdn = True
                    break

                # æå–ä¸»åŸŸå¹¶åˆ¤æ–­æ˜¯å¦è¢«è¿‡æ»¤
                ext = tldextract.extract(domain)
                root_domain = f"{ext.domain}.{ext.suffix}"
                if not any(fd in root_domain for fd in filter_domains):
                    ip_domain_map[ip_].append(domain)

            except Exception as e:
                if DEBUG_FSCAN:
                    print(f"[!] åŸŸåå­—ç¬¦ä¸²å¤„ç†å¼‚å¸¸: {e}")

        if is_cdn:
            continue  # é¿å…è®°å½•ä»»ä½•åŸŸå
    # âœ… å†™å…¥ CDN IP å¹¶æ›´æ–° a_records
    new_cdn_ips = cdn_ip_to_remove - existing_cdn_dyn_ips
    if new_cdn_ips:
        with open(CDN_DYNAMIC_PATH, "a", encoding="utf-8") as f:
            for ip in new_cdn_ips:
                f.write(f"{ip}\n")
        existing_cdn_dyn_ips.update(new_cdn_ips)
        update_a_records_after_scan(cdn_ip_to_remove, folder)

    return ip_domain_map, cdn_ip_to_remove

def extract_root_domain(domain):
    ext = tldextract.extract(domain)
    if ext.suffix:
        return f"{ext.domain}.{ext.suffix}"
    return domain
async def query_platform_by_hash(hash_value, platform="fofa", hash_type="icon_hash", size=100, proxies=None):
    """
    é€šç”¨ hash/title æŸ¥è¯¢æ¥å£ï¼Œæ”¯æŒ FOFA / Hunterï¼Œè¿”å›åŸŸååˆ—è¡¨ã€‚
    :param hash_value: hash å€¼ï¼ˆicon_hash / body_hashï¼‰æˆ–æ ‡é¢˜å†…å®¹
    :param platform: å¹³å°æ ‡è¯† "fofa" / "hunter"
    :param hash_type: æŸ¥è¯¢ç±»å‹ icon_hash / body_hash / cert / title (FOFA) æˆ– web.icon / web.title (Hunter)
    :param size: æœ€å¤§è¿”å›æ•°é‡ï¼ˆfofa ç”¨ï¼Œhunter å›ºå®šä¸€é¡µ 100ï¼‰
    :param proxies: ä»£ç† URL å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "socks5h://127.0.0.1:7891" æˆ– "http://127.0.0.1:7890"
    """
    assert platform in {"fofa", "hunter"}, "platform å¿…é¡»æ˜¯ 'fofa' æˆ– 'hunter'"

    if platform == "fofa":
        query = f'{hash_type}="{hash_value}"'
        qbase64 = base64.b64encode(query.encode()).decode()
        url = (
            f"https://fofa.info/api/v1/search/all?"
            f"email={TEST_EMAIL}&key={TEST_KEY}&qbase64={qbase64}"
            f"&size={size}&fields=host"
        )

        try:
            async with httpx.AsyncClient(timeout=10) as client:
                r = await client.get(url)
                r.raise_for_status()
                data = r.json()

                if data.get("error") is False:
                    results = data.get("results", [])
                    if not results:
                        # print(f"[!] FOFA ç©ºç»“æœ: {hash_type}={hash_value}")
                        return []
                    first_item = results[0]
                    if isinstance(first_item, list):
                        return list(set(row[0] for row in results if row))
                    elif isinstance(first_item, str):
                        return list(set(results))
                    else:
                        print(f"[!] FOFA æœªçŸ¥ç»“æœæ ¼å¼: {type(first_item)}")
                        return []
                else:
                    print(f"[!] FOFA é”™è¯¯: {data.get('errmsg')}")
                    return []

        except Exception as e:
            print(f"[!] æŸ¥è¯¢å¤±è´¥ (fofa): {e}")
            return []

    else:  # Hunter æŸ¥è¯¢
        if hash_type == "title":
            query = f'web.title="{hash_value}"'
        else:
            query = f'web.icon="{hash_value}"'
        start_time = time.strftime("%Y-%m-%d", time.localtime(time.time() - 30*24*3600))
        end_time = time.strftime("%Y-%m-%d", time.localtime())
        url = (
            f"https://hunter.qianxin.com/openApi/search?"
            f"api-key={HUNTER_API_KEY}&search={query}&start_time={start_time}&end_time={end_time}"
            f"&page=1&page_size=100&is_web=3"
        )

        try:
            async with httpx.AsyncClient(timeout=10, proxy=proxies) as client:
                r = await client.get(url)
                r.raise_for_status()
                data = r.json()

                if data.get("code") != 200:
                    print(f"[!] Hunter é”™è¯¯: {data.get('message')}")
                    return []

                results = data.get("data", {}).get("arr", [])
                return list({r.get("domain") for r in results if r.get("domain")})

        except Exception as e:
            print(f"[!] æŸ¥è¯¢å¤±è´¥ (hunter): {e}")
            return []
def is_ip(string):
    """æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä¸ºIPåœ°å€ï¼ˆæ”¯æŒå¸¦ç«¯å£çš„æ ¼å¼ï¼‰"""
    # ç§»é™¤ç«¯å£éƒ¨åˆ†
    if ':' in string:
        string = string.split(':')[0]
    return re.match(r"^\d{1,3}(\.\d{1,3}){3}$", string) is not None
def clean_line(line):
    return line.strip().strip('"').strip("'").lower()

async def read_lines_from_file(filepath):
    lines = set()
    if os.path.exists(filepath):
        async with aiofiles.open(filepath, mode='r') as f:
            async for line in f:
                line = clean_line(line)
                if line:
                    lines.add(line)
    return lines
async def write_lines_to_file(filepath, lines):
    if not lines:
        return
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    async with aiofiles.open(filepath, mode='a') as f:
        for line in sorted(lines):
            await f.write(line + '\n')
def parse_url(line):
    line = clean_line(line)
    if not line:
        return None, None
    if line.startswith('http://') or line.startswith('https://'):
        parsed = urlparse(line)
        hostname = parsed.hostname
        return line, hostname
    else:
        return None, line  # treat as domain or IP
def strip_url_scheme(url: str) -> str:
    """å»æ‰ http:// æˆ– https://ï¼Œåªè¿”å› host"""
    url = url.strip()
    if url.startswith("http://") or url.startswith("https://"):
        parsed = urlparse(url)
        return parsed.hostname or url  # fallback
    return url

async def merge_all_expanded_results(report_folder: str, root_domain: str, redirect_domains: set = None):
    tuozhan_path = os.path.join(report_folder, "tuozhan")
    all_dir = os.path.join(tuozhan_path, "all_tuozhan")
    os.makedirs(all_dir, exist_ok=True)

    existing_report_folder = f"./output/{root_domain}"
    existing_urls_raw = await read_lines_from_file(os.path.join(existing_report_folder, "input/urls.txt"))
    existing_urls_hosts = {strip_url_scheme(u) for u in existing_urls_raw}

    a_record_path = f"{existing_report_folder}/input/a_records.txt"
    existing_ips = await read_lines_from_file(a_record_path)

    # ä¿å­˜æ¥æºæ˜ å°„: {è¯¦ç»†æ¥æº: set(åŸŸå/IP)}
    source_host_map = defaultdict(set)

    # âœ… 1. å¤„ç† fofa å­ç›®å½•ä¸‹æ‰€æœ‰ txt æ–‡ä»¶
    for subfolder in ["fofa"]:
        full_path = os.path.join(tuozhan_path, subfolder)
        if not os.path.exists(full_path):
            continue

        for fname in os.listdir(full_path):
            if not fname.endswith(".txt"):
                continue

            file_path = os.path.join(full_path, fname)
            current_source = None
            domains = []

            async with aiofiles.open(file_path, mode='r') as f:
                async for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    if line.startswith("# æ¥æº:"):
                        original_source = line.replace("# æ¥æº:", "").strip()
                        # æ„å»ºè¯¦ç»†æ¥æº: "fofaçš„cert_vtmarkets.com.txt -> https://go.vtmarkets.com"
                        current_source = f"fofaçš„{fname} -> {original_source}"
                        continue
                    domain = clean_line(line)
                    if not domain:
                        continue
                    host = strip_url_scheme(domain)
                    if not host:
                        continue

                    if is_ip(host):
                        if host not in existing_ips:
                            source_host_map[current_source].add(host)
                    else:
                        if host not in existing_urls_hosts:
                            source_host_map[current_source].add(host)

    # âœ… 2. åˆå¹¶ root domains
    merged_roots = set()
    ip_re_path = os.path.join(tuozhan_path, "ip_re", "ip_domain_summary.txt")
    if os.path.exists(ip_re_path):
        async with aiofiles.open(ip_re_path, mode='r') as f:
            async for line in f:
                domain = clean_line(line)
                if not domain or is_ip(domain):
                    continue
                root = extract_root_domain(domain)
                if root and root not in existing_urls_hosts and root != root_domain:
                    merged_roots.add(root)

    # ğŸ†• æ·»åŠ ä»è·³è½¬å‘ç°çš„åŸŸå
    if redirect_domains:
        redirect_count = 0
        for redirect_domain in redirect_domains:
            if redirect_domain and redirect_domain not in existing_urls_hosts:
                # éªŒè¯åŸŸåæ ¼å¼å¹¶æ’é™¤ä¸ä¸»åŸŸåç›¸åŒçš„åŸŸå
                if (not is_ip(redirect_domain) and '.' in redirect_domain and 
                    redirect_domain != root_domain):
                    merged_roots.add(redirect_domain)
                    redirect_count += 1
        if redirect_count > 0:
            print(f"[+] ä»URLè·³è½¬å‘ç° {redirect_count} ä¸ªæ–°æ ¹åŸŸåï¼ˆå·²æ’é™¤ä¸ä¸»åŸŸåé‡å¤ï¼‰")

    # âœ… 3. é‡æ–°è®¾è®¡æ–‡ä»¶è¾“å‡ºæ ¼å¼ - æ‰€æœ‰æ–‡ä»¶éƒ½åŒ…å«æ¥æºä¿¡æ¯
    merged_ips_with_source = []  # [(ip, source), ...]
    merged_urls_with_source = []  # [(url, source), ...]
    merged_roots_with_source = []  # [(root_domain, source), ...]
    
    # æ·»åŠ è·³è½¬å‘ç°çš„æ ¹åŸŸåï¼ˆå¸¦æ¥æºæ ‡è¯†ï¼Œæ’é™¤ä¸»åŸŸåï¼‰
    if merged_roots:
        for root in merged_roots:
            if root != root_domain:
                merged_roots_with_source.append((root, "URLè·³è½¬å‘ç°"))
    
    for source, hosts in source_host_map.items():
        for host in hosts:
            if is_ip(host):
                merged_ips_with_source.append((host, source))
            else:
                # åˆ¤æ–­æ˜¯å¦ä¸ºä¸»åŸŸå
                root = extract_root_domain(host)
                if root and root == host:
                    # æ˜¯ä¸»åŸŸåï¼Œæ·»åŠ åˆ°root_domainsï¼ˆæ’é™¤ä¸å½“å‰æ‰«æä¸»åŸŸåé‡å¤çš„ï¼‰
                    if host != root_domain:
                        merged_roots_with_source.append((host, source))
                else:
                    # æ˜¯å­åŸŸåï¼Œæ·»åŠ åˆ°urls
                    merged_urls_with_source.append((host, source))
    
    # å†™å…¥ ip.txt - åªå­˜IPä½†æ ‡è¯†æ¥æº
    ip_txt_path = os.path.join(all_dir, "ip.txt")
    async with aiofiles.open(ip_txt_path, "w") as f:
        if merged_ips_with_source:
            current_source = None
            for ip, source in sorted(merged_ips_with_source, key=lambda x: (x[1], x[0])):
                if current_source != source:
                    current_source = source
                    await f.write(f"# æ¥æº: {source}\n")
                await f.write(f"{ip}\n")
        else:
            await f.write("# æš‚æ— IPç›®æ ‡\n")
    
    # å†™å…¥ urls.txt - åªå­˜å­åŸŸå/URLä½†æ ‡è¯†æ¥æº
    urls_txt_path = os.path.join(all_dir, "urls.txt")
    async with aiofiles.open(urls_txt_path, "w") as f:
        if merged_urls_with_source:
            current_source = None
            for url, source in sorted(merged_urls_with_source, key=lambda x: (x[1], x[0])):
                if current_source != source:
                    current_source = source
                    await f.write(f"# æ¥æº: {source}\n")
                await f.write(f"{url}\n")
        else:
            await f.write("# æš‚æ— URLç›®æ ‡\n")
    
    # å†™å…¥ root_domains.txt - æ‰€æœ‰ä¸»åŸŸåä½†æ ‡è¯†æ¥æº
    root_domains_path = os.path.join(all_dir, "root_domains.txt")
    async with aiofiles.open(root_domains_path, "w") as f:
        if merged_roots_with_source:
            current_source = None
            for root, source in sorted(merged_roots_with_source, key=lambda x: (x[1], x[0])):
                if current_source != source:
                    current_source = source
                    await f.write(f"# æ¥æº: {source}\n")
                await f.write(f"{root}\n")
        else:
            await f.write("# æš‚æ— æ ¹åŸŸåç›®æ ‡\n")

async def load_fofa_query_blacklist() -> set[str]:
    try:
        async with aiofiles.open(BLACKLIST_FILE_PATH, mode='r') as f:
            content = await f.read()
        return set(line.strip() for line in content.splitlines() if line.strip())
    except FileNotFoundError:
        return set()

async def save_fofa_query_blacklist(blacklist: set[str]):
    os.makedirs(os.path.dirname(BLACKLIST_FILE_PATH), exist_ok=True)
    async with aiofiles.open(BLACKLIST_FILE_PATH, mode='w') as f:
        for item in sorted(blacklist):
            await f.write(f"{item}\n")

async def write_expanded_reports(report_folder, ico_mmh3_set=None, body_mmh3_set=None, domain_list=None, use_hunter=False, hunter_proxies=None, hunter_ico_md5_list=None, cert_root_domains=None, cert_root_domain_map=None, ico_md5_url_map=None, ico_mmh3_url_map=None, body_md5_url_map=None, body_mmh3_url_map=None, title_set=None, title_url_map=None, enable_fofa: bool = True):

    tuozhan_dir = Path(report_folder) / "tuozhan"
    fofa_dir = tuozhan_dir / "fofa"
    ip_re_dir = tuozhan_dir / "ip_re"
    all_tuozhan_dir = tuozhan_dir / "all_tuozhan"
    tuozhan_dir.mkdir(parents=True, exist_ok=True)
    fofa_dir.mkdir(parents=True, exist_ok=True)
    ip_re_dir.mkdir(parents=True, exist_ok=True)
    all_tuozhan_dir.mkdir(parents=True, exist_ok=True)
    updated_blacklist = set()
    fofa_blacklist = await load_fofa_query_blacklist()

    if use_hunter:
        hunter_dir = tuozhan_dir / "hunter"
        hunter_dir.mkdir(parents=True, exist_ok=True)

    if ico_mmh3_set:
        for hash_value in sorted(ico_mmh3_set):
            if use_hunter:
                if not hunter_ico_md5_list:
                    print(f"[!] Hunter æŸ¥è¯¢éœ€è¦ä¼ å…¥ ico md5 åˆ—è¡¨ï¼Œå½“å‰ä¸ºç©ºï¼Œè·³è¿‡ icon_hash={hash_value}")
                    continue
                for md5_hash in hunter_ico_md5_list:
                    print(f"[+] æŸ¥è¯¢ HUNTER icon md5={md5_hash}")
                    try:
                        domains = await query_platform_by_hash(
                            md5_hash,
                            platform="hunter",
                            hash_type="icon_md5",
                            proxies=hunter_proxies
                        )
                        updated_blacklist.add(md5_hash)
                    except Exception as e:
                        print(f"[!] Hunter æŸ¥è¯¢å¤±è´¥: {e}")
                        continue
                    if not domains:
                        continue
                    file_path = hunter_dir / f"icon_md5_hunter_{md5_hash}.txt"
                    with open(file_path, "w", encoding="utf-8") as f:
                        if ico_md5_url_map and md5_hash in ico_md5_url_map:
                            for src in sorted(ico_md5_url_map[md5_hash]):
                                f.write(f"# æ¥æº: {src}\n")
                        for domain in domains:
                            f.write(f"{domain}\n")
            else:
                if enable_fofa:
                    if hash_value in fofa_blacklist:
                        print(f"[!] è·³è¿‡ FOFA æŸ¥è¯¢ (é»‘åå•): icon_hash={hash_value}")
                        continue
                    print(f"[+] æŸ¥è¯¢ FOFA icon_hash={hash_value}")
                    try:
                        domains = await query_platform_by_hash(
                            hash_value,
                            platform="fofa",
                            hash_type="icon_hash"
                        )
                        updated_blacklist.add(hash_value)
                    except Exception as e:
                        print(f"[!] FOFA æŸ¥è¯¢å¤±è´¥: {e}")
                        continue
                    if not domains:
                        continue
                    file_path = fofa_dir / f"icon_hash_{hash_value}.txt"
                    with open(file_path, "w", encoding="utf-8") as f:
                        if ico_mmh3_url_map and hash_value in ico_mmh3_url_map:
                            for src in sorted(ico_mmh3_url_map[hash_value]):
                                f.write(f"# æ¥æº: {src}\n")
                        for domain in domains:
                            f.write(f"{domain}\n")

    if body_mmh3_set and enable_fofa:
        for hash_value in sorted(body_mmh3_set):
            if hash_value in fofa_blacklist:
                print(f"[!] è·³è¿‡ FOFA æŸ¥è¯¢ (é»‘åå•): body_hash={hash_value}")
                continue
            print(f"[+] æŸ¥è¯¢ FOFA body_hash={hash_value}")
            try:
                domains = await query_platform_by_hash(
                    hash_value,
                    platform="fofa",
                    hash_type="body_hash"
                )
                updated_blacklist.add(hash_value)
            except Exception as e:
                print(f"[!] FOFA æŸ¥è¯¢å¤±è´¥: {e}")
                continue
            if not domains:
                continue
            file_path = fofa_dir / f"body_hash_{hash_value}.txt"
            with open(file_path, "w", encoding="utf-8") as f:
                if body_mmh3_url_map and hash_value in body_mmh3_url_map:
                    for src in sorted(body_mmh3_url_map[hash_value]):
                        f.write(f"# æ¥æº: {src}\n")
                for domain in domains:
                    f.write(f"{domain}\n")

    if cert_root_domains and enable_fofa:
        for domain in sorted(cert_root_domains):
            if domain in fofa_blacklist:
                print(f"[!] è·³è¿‡ FOFA æŸ¥è¯¢ (é»‘åå•): cert={domain}")
                continue
            print(f"[+] æŸ¥è¯¢ FOFA cert={domain}")
            try:
                domains = await query_platform_by_hash(
                    domain,
                    platform="fofa",
                    hash_type="cert"
                )
                updated_blacklist.add(domain)
            except Exception as e:
                print(f"[!] FOFA æŸ¥è¯¢å¤±è´¥: cert={domain} é”™è¯¯: {e}")
                continue
            if not domains:
                continue
            file_path = fofa_dir / f"cert_{domain}.txt"
            with open(file_path, "w", encoding="utf-8") as f:
                if cert_root_domain_map and domain in cert_root_domain_map:
                    for src in sorted(cert_root_domain_map[domain]):
                        f.write(f"# æ¥æº: {src}\n")
                for d in domains:
                    f.write(f"{d}\n")

    # æ·»åŠ æ ‡é¢˜æœç´¢åŠŸèƒ½
    if title_set and enable_fofa:
        for title in sorted(title_set):
            if title in fofa_blacklist:
                print(f"[!] è·³è¿‡ FOFA æŸ¥è¯¢ (é»‘åå•): title={title}")
                continue
            print(f"[+] æŸ¥è¯¢ FOFA title={title}")
            try:
                domains = await query_platform_by_hash(
                    title,
                    platform="fofa",
                    hash_type="title"
                )
                updated_blacklist.add(title)
            except Exception as e:
                print(f"[!] FOFA æŸ¥è¯¢å¤±è´¥: title={title} é”™è¯¯: {e}")
                continue
            if not domains:
                continue
            file_path = fofa_dir / f"title_{title[:50]}.txt"  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
            with open(file_path, "w", encoding="utf-8") as f:
                if title_url_map and title in title_url_map:
                    for src in sorted(title_url_map[title]):
                        f.write(f"# æ¥æº: {src}\n")
                for domain in domains:
                    f.write(f"{domain}\n")

    # Hunter æ ‡é¢˜æœç´¢
    if use_hunter and title_set:
        hunter_dir = tuozhan_dir / "hunter"
        hunter_dir.mkdir(parents=True, exist_ok=True)
        
        for title in sorted(title_set):
            print(f"[+] æŸ¥è¯¢ HUNTER title={title}")
            try:
                domains = await query_platform_by_hash(
                    title,
                    platform="hunter",
                    hash_type="title",
                    proxies=hunter_proxies
                )
                updated_blacklist.add(title)
            except Exception as e:
                print(f"[!] Hunter æŸ¥è¯¢å¤±è´¥: title={title} é”™è¯¯: {e}")
                continue
            if not domains:
                continue
            file_path = hunter_dir / f"title_hunter_{title[:50]}.txt"
            with open(file_path, "w", encoding="utf-8") as f:
                if title_url_map and title in title_url_map:
                    for src in sorted(title_url_map[title]):
                        f.write(f"# æ¥æº: {src}\n")
                for domain in domains:
                    f.write(f"{domain}\n")

    if domain_list:
        root_domains = {extract_root_domain(d) for d in domain_list if d}
        if root_domains:
            out_file = ip_re_dir / "ip_domain_summary.txt"
            with open(out_file, "w", encoding="utf-8") as f:
                for domain in sorted(root_domains):
                    f.write(f"{domain}\n")

    print("[+] å®ŒæˆæŸ¥è¯¢,å¼€å§‹æ±‡æ€»å†™å…¥æ–‡ä»¶")
    await save_fofa_query_blacklist(fofa_blacklist.union(updated_blacklist))



async def write_base_report(root: str, report_folder: Path, valid_ips: set[str], urls: list[str], titles: dict, ip_domain_map: dict[str, list[str]], url_body_info_map: dict[str, dict], redirect_domains: set = None):

    all_icos = set()
    all_body_hashes = set()
    all_certs = set()
    all_icos_mmh3 = set()
    all_body_mmh3 = set()
    all_reverse_domains = []
    all_titles = set()

    ico_md5_url_map = defaultdict(set)
    ico_mmh3_url_map = defaultdict(set)
    body_md5_url_map = defaultdict(set)
    body_mmh3_url_map = defaultdict(set)
    cert_root_url_map = defaultdict(set)
    title_url_map = defaultdict(set)

    repeat_map = defaultdict(list)

    indent1 = "  "
    indent2 = "    "

    out_path = report_folder / f"base_info_{root}.txt"
    with open(out_path, "w", encoding="utf-8") as out:
        out.write(f"{'='*30}\n[åŸºç¡€ä¿¡æ¯æ±‡æ€»] åŸŸå: {root}\n{'='*30}\n")

        # === 1. å…³è”IP ===
        out.write("å…³è”çœŸå®IP:\n")
        for ip in sorted(valid_ips):
            out.write(f"{indent1}- {ip}\n")

        # === 2. URL æ ‡é¢˜ä¿¡æ¯ & hash åˆ†ç±» ===
        out.write("\nURLå’Œæ ‡é¢˜:\n")
        for url in urls:
            title, cert, ico, body_hash, url_ips, ico_mmh3, bd_mmh3 = titles.get(url, ("", "", "", "", (), "", ""))
            key = (body_hash, cert, ",".join(sorted(url_ips)), ico, ico_mmh3, bd_mmh3)
            repeat_map[key].append((url, title, cert, ico, body_hash, ico_mmh3, bd_mmh3))

        for url_list in repeat_map.values():
            for i, (url, title, cert, ico, body_hash, ico_mmh3, bd_mmh3) in enumerate(url_list):
                if i > 0:
                    continue
                out.write(f"{indent1}- {url} [{title}]\n")
                if ico:
                    all_icos.add(ico)
                    ico_md5_url_map[ico].add(url)
                if body_hash:
                    all_body_hashes.add(body_hash)
                    body_md5_url_map[body_hash].add(url)
                if ico_mmh3:
                    all_icos_mmh3.add(ico_mmh3)
                    ico_mmh3_url_map[ico_mmh3].add(url)
                if bd_mmh3:
                    all_body_mmh3.add(bd_mmh3)
                    body_mmh3_url_map[bd_mmh3].add(url)
                if cert and cert.strip():
                    root_domain = extract_root_domain(cert.strip("*."))  # <-- éœ€ç¡®ä¿æ­¤å‡½æ•°å­˜åœ¨
                    if root_domain:
                        all_certs.add(cert)
                        cert_root_url_map[root_domain].add(url)
                if title and title.strip() and title not in black_titles:
                    all_titles.add(title.strip())
                    title_url_map[title.strip()].add(url)

        # === 3. IPåæŸ¥åŸŸå ===
        out.write("\nIPåæŸ¥åŸŸå:\n")
        for ip in sorted(valid_ips):
            if ip in ip_domain_map:
                out.write(f"{indent1}[IP] {ip}\n")
                for domain in ip_domain_map[ip]:
                    all_reverse_domains.append(domain)
                    out.write(f"{indent2}- {domain}\n")

        # === 4. URL body info ä¸­æŠ½å–çš„åŸŸå ===
        urls_for_root = [url for url in urls if url_body_info_map.get(url)]
        if urls_for_root:
            out.write(f"\n[URL BODY INFO - åŸŸå(ç›®å‰éœ€è¦æ‰‹åŠ¨ç­›é€‰): {root}]\n")
            url_domains_seen = {urlparse(url).hostname for url in urls_for_root if urlparse(url).hostname}
            domain_source_map = defaultdict(set)
            for url in urls_for_root:
                info = url_body_info_map.get(url, {})
                for d in info.get("body_fqdn", []) + info.get("body_domains", []):
                    if d not in url_domains_seen:
                        domain_source_map[d].add(url)

            for domain, source_urls in domain_source_map.items():
                if len(source_urls) == 1:
                    out.write(f"{indent1}{domain} [æ¥æº: {next(iter(source_urls))}]\n")
                else:
                    out.write(f"{indent1}{domain} [æ¥æºæ•°é‡: {len(source_urls)}]\n")

        # === 5. hash / cert æ±‡æ€» ===
        out.write(f"\n{'='*30}\nèµ„æºæ±‡æ€»:\n{'='*30}\n")
        out.write("\nè¯ä¹¦ä¸»åŸŸå:\n")
        for cert_domain in sorted(cert_root_url_map.keys()):
            out.write(f"{indent1}{cert_domain}\n")
        out.write("ico:\n")
        out.write(f"{indent1}md5:\n")
        for ico in sorted(all_icos):
            out.write(f"{indent2}{ico}\n")
        out.write(f"{indent1}mmh3_hash:\n")
        for h in sorted(all_icos_mmh3):
            out.write(f"{indent2}{h}\n")

        out.write("\nbody_hash:\n")
        out.write(f"{indent1}md5:\n")
        for h in sorted(all_body_hashes):
            out.write(f"{indent2}{h}\n")
        out.write(f"{indent1}mmh3_hash:\n")
        for h in sorted(all_body_mmh3):
            out.write(f"{indent2}{h}\n")



        out.write("\nasnä¿¡æ¯(æš‚æœªå®ç°):\n")

        # === 6. é‡å¤é¡µé¢èšç±» ===
        out.write(f"\n{'='*30}\né‡å¤ç½‘ç«™:\n{'='*30}\n\n")
        indent3 = indent2 * 2
        for key, url_infos in repeat_map.items():
            if len(url_infos) > 1:
                main_url, main_title, *_ = url_infos[0]
                out.write(f"{indent1}- é‡å¤äº: {main_url}  æ ‡é¢˜: {main_title}\n")
                for url, title, *_ in url_infos:
                    out.write(f"{indent2}- {url}\n")
                    out.write(f"{indent3}æ ‡é¢˜: {title}\n")

    # === 7. å†™å…¥æ‰©å±•æŸ¥è¯¢ç»“æœï¼ˆFOFA / hunterï¼‰===
    if all_reverse_domains or all_icos_mmh3 or all_body_mmh3 or cert_root_url_map or all_titles:
        await write_expanded_reports(
            report_folder=report_folder,
            ico_mmh3_set=all_icos_mmh3,
            body_mmh3_set=all_body_mmh3,
            domain_list=all_reverse_domains,
            use_hunter=False,
            hunter_proxies=None,
            hunter_ico_md5_list=all_icos,
            cert_root_domains=set(cert_root_url_map.keys()),
            cert_root_domain_map=cert_root_url_map,
            ico_md5_url_map=ico_md5_url_map,
            ico_mmh3_url_map=ico_mmh3_url_map,
            body_md5_url_map=body_md5_url_map,
            body_mmh3_url_map=body_mmh3_url_map,
            title_set=all_titles,
            title_url_map=title_url_map,
            enable_fofa=True

        )

    # === 8. æ±‡æ€» merge æŠ¥å‘Š ===ï¼ˆç§»åˆ°æ¡ä»¶å¤–ï¼Œç¡®ä¿æ€»æ˜¯æ‰§è¡Œï¼‰
    await merge_all_expanded_results(report_folder, root, redirect_domains)


async def write_representative_urls(folder, titles, urls):
    repeat_map = defaultdict(list)
    for url in urls:
        title, cert, ico, body_hash, url_ips, ico_mmh3, bd_mmh3 = titles.get(url, ("", "", "", "", (), "", ""))
        a_str = ",".join(sorted(url_ips))
        key = (body_hash, cert, a_str, ico)
        repeat_map[key].append((url, title, cert, ico, body_hash, ico_mmh3, bd_mmh3))

    input_folder = folder / "input"
    input_folder.mkdir(exist_ok=True)
    path = input_folder / "representative_urls.txt"
    with open(path, "w", encoding="utf-8") as f:
        for url_list in repeat_map.values():
            if url_list:
                url, title, *_ = url_list[0]
                if title in black_titles:
                    continue
                f.write(url + "\n")


async def run_security_scans(root, folder, report_folder):
    afrog_report = report_folder / f"afrog_report_{root}.json"
    fscan_report = report_folder / f"fscan_result_{root}.txt"
    afrog_target_file = folder / "input" / "representative_urls.txt"
    fscan_target_file = folder / "input" / "a_records.txt"
    if not afrog_target_file.exists() or os.path.getsize(afrog_target_file) == 0:
        empty_file = report_folder / "afrogç›®æ ‡ä¸ºç©º.txt"
        empty_file.touch()  # åˆ›å»ºç©ºæ–‡ä»¶
        print(f"[!] {afrog_target_file} ä¸ºç©ºï¼Œå·²åˆ›å»º {empty_file}ï¼Œè·³è¿‡afrogæ‰«æ")
    else:
        afrog_cmd = AFROG_CMD_TEMPLATE.format(target_file=str(afrog_target_file), output_file=str(afrog_report))
        result = await run_cmd_async(afrog_cmd)
        if result is None:
            print(f"[!] afrogæ‰«æå¤±è´¥ï¼Œè·³è¿‡")
            return

    if not fscan_target_file.exists() or os.path.getsize(fscan_target_file) == 0:
        empty_file = report_folder / "fscanç›®æ ‡ä¸ºç©º.txt"
        empty_file.touch()
        print(f"[!] {fscan_target_file} ä¸ºç©ºï¼Œå·²åˆ›å»º {empty_file}ï¼Œè·³è¿‡fscanæ‰«æ")
    else:
        fscan_cmd = FSCAN_CMD_TEMPLATE.format(target_file=str(fscan_target_file), output_file=str(fscan_report))
        result = await run_cmd_async(fscan_cmd)
        if result is None:
            print(f"[!] fscanæ‰«æå¤±è´¥ï¼Œè·³è¿‡")
            return
    await finalize_report_directory(report_folder, root)


async def finalize_report_directory(report_folder, root):
    afrog_report = report_folder / f"afrog_report_{root}.json"
    
    # æ£€æŸ¥afrogæŠ¥å‘Šæ˜¯å¦å­˜åœ¨ä¸”æœ‰æ¼æ´å†…å®¹
    has_vulns = False
    if afrog_report.exists():
        try:
            with open(afrog_report, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content and content != "[]":  # ä¸æ˜¯ç©ºæ•°ç»„
                    has_vulns = True
                    print(f"[+] æ£€æµ‹åˆ°afrogæ¼æ´æŠ¥å‘Š: {afrog_report}")
        except Exception as e:
            print(f"[!] è¯»å–afrogæŠ¥å‘Šå¤±è´¥: {e}")
    
    # ä½¿ç”¨ç®€åŒ–è¾“å‡ºç»“æ„ï¼Œä¸å†é‡å‘½åå¤æ‚ç›®å½•
    print(f"[*] åˆ›å»ºç®€åŒ–è¾“å‡ºç»“æ„...")
    simplified_folder = create_simplified_output(root, report_folder)
    
    # å†™å…¥æ‰«æå®Œæˆæ ‡å¿—
    scan_done_path = simplified_folder / "finish.txt"
    scan_done_path.write_text("æ‰«æå·²å®Œæˆ", encoding="utf-8")
    
    # å¦‚æœå‘ç°æ¼æ´ï¼Œåœ¨æ–‡ä»¶åä¸­æ ‡è®°
    if has_vulns:
        vuln_marker = simplified_folder / "å‘ç°æ¼æ´.txt"
        vuln_marker.write_text("æ£€æµ‹åˆ°å®‰å…¨æ¼æ´", encoding="utf-8")
        print(f"[!] å‘ç°æ¼æ´ï¼Œå·²æ ‡è®°: {vuln_marker}")
    
    print(f"[âœ“] æ‰«æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {simplified_folder}")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸ç®€åŒ–è¾“å‡ºä¸åŒï¼‰
    if str(report_folder.resolve()) != str(simplified_folder.resolve()):
        try:
            shutil.rmtree(report_folder)
            print(f"[âœ“] æ¸…ç†ä¸´æ—¶ç›®å½•: {report_folder}")
        except Exception as e:
            print(f"[!] æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")


def save_non_200_urls_by_domain(non_200_urls_all, url_root_map):
    # å¤„ç†ç‰¹æ®ŠçŠ¶æ€ç ï¼š403, 404ç­‰
    status_folders = [403, 404, 500, 502, 503]  # æ‰©å±•å…³æ³¨çš„çŠ¶æ€ç 
    # æŒ‰åŸŸåå’ŒçŠ¶æ€ç åˆ†ç»„ï¼š {domain: {status_code: [urls]}}
    domain_status_urls = defaultdict(lambda: defaultdict(list))

    for url, status_code in non_200_urls_all:
        if status_code in status_folders:
            root_domain = url_root_map.get(url)
            if root_domain:
                domain_status_urls[root_domain][status_code].append(url)

    # å†™å…¥æ–‡ä»¶ï¼ŒæŒ‰çŠ¶æ€ç åˆ†åˆ«ä¿å­˜åˆ°inputç›®å½•
    for domain, status_dict in domain_status_urls.items():
        domain_folder = Path("output") / domain
        input_folder = domain_folder / "input"
        input_folder.mkdir(parents=True, exist_ok=True)
        for status_code, urls in status_dict.items():
            file_path = input_folder / f"{status_code}_urls.txt"  # åŠ¨æ€æ–‡ä»¶å
            with open(file_path, "w", encoding="utf-8") as f:  # æ”¹ä¸ºwæ¨¡å¼é¿å…é‡å¤
                f.write(f"# {status_code}çŠ¶æ€ç URLåˆ—è¡¨ - {domain}\n")
                f.write(f"# æ€»è®¡: {len(urls)} ä¸ªURL\n\n")
                for u in urls:
                    f.write(u + "\n")


# ------------------------------------
# ä¸»ç¨‹åºå…¥å£
# ------------------------------------
# ä¸»ç¨‹åºå…¥å£
def main():
    init_dirs()
    filter_domains = load_filter_domains(FILTER_DOMAIN_PATH)
    cdn_ranges = load_cdn_ranges(CDN_LIST_PATH)
    existing_cdn_dyn_ips = {line.strip() for line in open(CDN_DYNAMIC_PATH, encoding="utf-8")} if os.path.exists(CDN_DYNAMIC_PATH) else set()

    if not os.path.exists(RESULT_JSON_PATH):
        print("[X] ç»“æœæ–‡ä»¶ä¸å­˜åœ¨")
        return

    with open(RESULT_JSON_PATH, "r", encoding="utf-8") as f:
        lines = f.readlines()

    print("[*] å¼€å§‹å¤šè¿›ç¨‹è§£æ JSON è®°å½•...")
    cpu_count = min(multiprocessing.cpu_count(), 8)  # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°
    chunk_size = max(500, len(lines) // (cpu_count * 2))  # åŠ¨æ€è°ƒæ•´chunkå¤§å°
    chunks = list(chunked_iterable(lines, chunk_size))
    
    print(f"[*] ä½¿ç”¨ {cpu_count} ä¸ªè¿›ç¨‹ï¼Œ{len(chunks)} ä¸ªchunkï¼Œæ¯ä¸ªchunkçº¦ {chunk_size} è¡Œ")

    worker = partial(parse_json_lines_chunk,
                     cdn_ranges=cdn_ranges,
                     existing_cdn_dyn_ips=existing_cdn_dyn_ips,
                     filter_domains=filter_domains)

    pool = multiprocessing.Pool(cpu_count)

    domain_ip_map = defaultdict(set)
    url_title_list = []
    url_root_map = {}
    url_body_info_map = {}  # âœ… æ–°å¢
    non_200_urls_all = []  # æ–°å¢ï¼Œå­˜å‚¨æ‰€æœ‰é200/301/302 url
    redirect_domains_all = set()  # æ–°å¢ï¼Œå­˜å‚¨æ‰€æœ‰è·³è½¬å‘ç°çš„åŸŸå

    with tqdm(total=len(chunks), desc="å¤„ç†è®°å½•") as pbar:
        for dmap, titles, urlmap, url_body_info, non_200_urls, redirect_domains in pool.imap_unordered(worker, chunks):
            for k, v in dmap.items():
                domain_ip_map[k].update(v)
            url_title_list.extend(titles)
            url_root_map.update(urlmap)
            url_body_info_map.update(url_body_info)  # âœ… åˆå¹¶è¿‡æ»¤åæ•°æ®
            non_200_urls_all.extend(non_200_urls)
            redirect_domains_all.update(redirect_domains)  # åˆå¹¶è·³è½¬åŸŸå

            pbar.update(1)

    pool.close()
    pool.join()
    # å‡†å¤‡æŒ‰åŸŸååˆ†ç»„ urls å’Œ titles
    domain_urls_map = defaultdict(set)
    domain_titles_map = {}
    for url, root_domain in url_root_map.items():
        domain_urls_map[root_domain].add(url)

    for url, title, cert, ico, body, url_ips,ico_mmh3,bd_mmh3 in url_title_list:
        domain_titles_map[url] = (title, cert, ico, body, url_ips,ico_mmh3,bd_mmh3)

    #403
    save_non_200_urls_by_domain(non_200_urls_all, url_root_map)


    
    # å¼‚æ­¥ä»»åŠ¡æ”¾åˆ° asyncio.run ä¸­æ‰§è¡Œ
    asyncio.run(run_domain_tasks(domain_ip_map, domain_urls_map, domain_titles_map, cdn_ranges, filter_domains, existing_cdn_dyn_ips, url_body_info_map, redirect_domains_all))


async def run_domain_tasks(domain_ip_map, domain_urls_map, domain_titles_map, cdn_ranges, filter_domains, existing_cdn_dyn_ips, url_body_info_map, redirect_domains=None):
    global SKIP_CURRENT_DOMAIN
    print("[*] å¼€å§‹é€ä¸ªæ‰§è¡ŒåŸŸåæµç¨‹...")
    sorted_domains = sorted(domain_urls_map.keys(), key=natural_sort_key)

    for domain in sorted_domains:
        if SKIP_CURRENT_DOMAIN:
            print(f"[!] è·³è¿‡åŸŸå: {domain}")
            SKIP_CURRENT_DOMAIN = False
            continue

        try:
            ips = domain_ip_map[domain]
            urls = sorted(domain_urls_map.get(domain, []))
            titles = {u: domain_titles_map.get(u, ("", "", "", "", (), "", "")) for u in urls}
            await per_domain_flow_sync_async(domain, ips, urls, titles, cdn_ranges, filter_domains, existing_cdn_dyn_ips, url_body_info_map, redirect_domains)
        except asyncio.CancelledError:
            print(f"[!] å½“å‰ä»»åŠ¡è¢«å–æ¶ˆ: {domain}")
            continue
        except Exception as e:
            print(f"[!] æ‰§è¡Œ {domain} å‡ºé”™: {e}")


# ------------------------------------
if __name__ == "__main__":
    signal.signal(signal.SIGINT, handle_sigint)   # Ctrl+C
    signal.signal(signal.SIGQUIT, handle_sigquit) # Ctrl+\
    main()
