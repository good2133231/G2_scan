import sys
import ipaddress
import os
import json
import subprocess
from pathlib import Path
import httpx
import time
from urllib.parse import urlparse
from collections import defaultdict
from tld import get_fld
from rapiddns import RapidDns
from tqdm import tqdm
import asyncio
import socket
from concurrent.futures import ThreadPoolExecutor
import multiprocessing
from itertools import islice
from functools import partial
import shutil
import tldextract
import re
import signal
import requests
from datetime import datetime
import random
import base64
import configparser
import aiofiles

# ------------------------------------
# å‘½ä»¤æ¨¡æ¿å’Œé…ç½®
# é¦–å…ˆè·å–é¡¹ç›®æ ¹ç›®å½•
# ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç›¸å¯¹è·¯å¾„æ¨å¯¼
if 'SCAN_PROJECT_ROOT' in os.environ:
    PROJECT_ROOT = os.environ['SCAN_PROJECT_ROOT']
else:
    # Fallback: ä»è„šæœ¬ä½ç½®æ¨å¯¼é¡¹ç›®æ ¹ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    PROJECT_ROOT = os.path.abspath(os.path.join(script_dir, '../..'))

# PROJECT_ROOT åˆå§‹åŒ–å®Œæˆ
# è·å–å·¥å…·è·¯å¾„
TOOLS_PATH = os.path.join(PROJECT_ROOT, "tools/scanner")

if '-small' in sys.argv or '-test' in sys.argv:
    print("[*] ä½¿ç”¨æµ‹è¯•ç¯å¢ƒå‘½ä»¤æ¨¡æ¿")
    AFROG_CMD_TEMPLATE = f"{TOOLS_PATH}/afrog -T {{target_file}} -c 100 -rl 300 -timeout 2 -s spring -doh -json {{output_file}}"
    FSCAN_CMD_TEMPLATE = f"{TOOLS_PATH}/fscan -hf {{target_file}} -p 80 -np -nobr -t 600 -o {{output_file}}"
    DEBUG_FSCAN = True
else:
    print("[*] ä½¿ç”¨æ­£å¼ç¯å¢ƒå‘½ä»¤æ¨¡æ¿")
    AFROG_CMD_TEMPLATE = f"{TOOLS_PATH}/afrog -T {{target_file}} -c 100 -rl 300 -timeout 2 -S high,info -doh -json {{output_file}}"
    FSCAN_CMD_TEMPLATE = f"{TOOLS_PATH}/fscan -hf {{target_file}} -p all -np -nobr -t 600  -o {{output_file}}"
    DEBUG_FSCAN = True
ONLY_DOMAIN_MODE = '-test' in sys.argv
RESULT_JSON_PATH = "temp/result_all.json"

if ONLY_DOMAIN_MODE:

    print("[*] ä»…å¤„ç†åŸŸåæ¨¡å¼ (-test)ï¼Œå°†è·³è¿‡å®‰å…¨æ‰«æä»»åŠ¡")
SKIP_CURRENT_DOMAIN = False

# ä½¿ç”¨ç¯å¢ƒå˜é‡è·å–é…ç½®æ–‡ä»¶è·¯å¾„
CDN_LIST_PATH = os.path.join(PROJECT_ROOT, "config/filters/cdn.txt")
CDN_DYNAMIC_PATH = os.path.join(PROJECT_ROOT, "config/filters/cdn_åŠ¨æ€æ·»åŠ _ä¸€å¹´æ¸…ä¸€æ¬¡.txt")
DYNAMIC_FILTER_FILE = Path(os.path.join(PROJECT_ROOT, "config/filters/filter_domains-åŠ¨æ€.txt"))
new_filtered_domains = set()

black_titles = {
        "Just a moment...",
        "Attention Required! | Cloudflare",
        "å®‰å…¨éªŒè¯",  # å¯æ ¹æ®ä½ ä¸šåŠ¡æ·»åŠ æ›´å¤šæ— æ•ˆæ ‡é¢˜
}
# 1. è¯»å–å·²æœ‰çš„åŠ¨æ€è¿‡æ»¤åŸŸå
# âœ… åŒæ­¥è¯»å–æ–¹å¼ï¼Œæœ€ç®€å•ç¨³å®šï¼ˆæ¨èç”¨äºéasyncç¨‹åºï¼‰
if DYNAMIC_FILTER_FILE.exists():
    with open(DYNAMIC_FILTER_FILE, mode='r', encoding='utf-8') as f:
        for line in f:
            line = line.strip().strip('"').strip("'").lower()
            if line:
                new_filtered_domains.add(line)


#è¿‡æ»¤
FILTER_DOMAIN_PATH = os.path.join(PROJECT_ROOT, "config/filters/filter-domain.txt")
BLACKLIST_FILE_PATH = os.path.join(PROJECT_ROOT, "config/filters/fofa_query_blacklist.txt")


hunter_proxies = "socks5h://127.0.0.1:7891"
config_path = Path(os.path.join(PROJECT_ROOT, "config/api/config.ini"))
config = configparser.ConfigParser()
config.read(config_path, encoding='utf-8')

TEST_EMAIL = config['DEFAULT'].get('TEST_EMAIL')
TEST_KEY = config['DEFAULT'].get('TEST_KEY')
HUNTER_API_KEY = ""

dns_cache = {}
reverse_lookup_semaphore = None  # å°†åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­åˆå§‹åŒ–

def handle_sigint(signum, frame):
    global SKIP_CURRENT_DOMAIN
    print("\n[!] æ”¶åˆ° Ctrl+Cï¼Œè·³è¿‡å½“å‰åŸŸåï¼Œç»§ç»­ä¸‹ä¸€ä¸ª...")
    SKIP_CURRENT_DOMAIN = True
def headers_lib():
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36"
    }
def handle_sigquit(signum, frame):
    print("\n[!] æ”¶åˆ° Ctrl+\\ï¼Œç»ˆæ­¢æ•´ä¸ªç¨‹åº")
    sys.exit(0)
def is_domain_resolvable(domain):
    if domain in dns_cache:
        return dns_cache[domain]
    try:
        socket.gethostbyname(domain)
        dns_cache[domain] = True
        return True
    except Exception:
        dns_cache[domain] = False
        return False
# ------------------------------------
async def reverse_lookup_ip_async(ip):
    """IPåæŸ¥å‡½æ•°ï¼Œå¸¦è¶…æ—¶å’Œé”™è¯¯å¤„ç†"""
    print(f"[>] å¼€å§‹åæŸ¥IP: {ip}")
    
    # æ–¹æ³•1: dnsdblookup
    try:
        print(f"[>] ä½¿ç”¨ dnsdblookup åæŸ¥åŸŸåæ¥å£: {ip}")
        url_d = f"https://dnsdblookup.com/{ip}/"
        async with httpx.AsyncClient(timeout=10) as client:
            res = await client.get(url_d, headers=headers_lib())
        site = re.findall(r'<span class="date">(.*?)</span><a href="/(.*?)/" target="_blank">(.*?)</a>', res.text, re.S)

        domains = [domain for _, _, domain in site]
        domains = list(set(domains))

        if domains:
            print(f"[âœ“] dnsdblookup æˆåŠŸåæŸ¥åˆ° {len(domains)} ä¸ªåŸŸå: {ip}")
            return ip, domains
    except Exception as e:
        print(f"[!] dnsdblookup åæŸ¥å¤±è´¥: {ip} - {e}")

    # æ–¹æ³•2: RapidDns (åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼Œé¿å…é˜»å¡)
    try:
        print(f"[>] ä½¿ç”¨ RapidDns åæŸ¥åŸŸåæ¥å£: {ip}")
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = loop.run_in_executor(executor, RapidDns.sameip, ip)
            try:
                domains = await asyncio.wait_for(future, timeout=15)  # 15ç§’è¶…æ—¶
                if domains:
                    # æ ¼å¼ç»Ÿä¸€ä¸ºæ‰å¹³åŒ–å­—ç¬¦ä¸²åˆ—è¡¨
                    flat_domains = []
                    for item in domains:
                        if isinstance(item, (list, tuple)):
                            flat_domains.append(item[0] if item else "")
                        else:
                            flat_domains.append(str(item))
                    
                    flat_domains = [d for d in flat_domains if d.strip()]
                    flat_domains = list(set(flat_domains))
                    
                    if flat_domains:
                        print(f"[âœ“] RapidDns æˆåŠŸåæŸ¥åˆ° {len(flat_domains)} ä¸ªåŸŸå: {ip}")
                        return ip, flat_domains
            except asyncio.TimeoutError:
                print(f"[!] RapidDns åæŸ¥è¶…æ—¶(15ç§’): {ip}")
    except Exception as e:
        print(f"[!] RapidDns åæŸ¥å¤±è´¥: {ip} - {e}")

    # æ–¹æ³•3: ip138 (å¤‡ç”¨æ–¹æ¡ˆ)
    try:
        print(f"[>] ä½¿ç”¨ ip138 åæŸ¥åŸŸåæ¥å£: {ip}")
        url_d_138 = f"https://ip138.com/{ip}/"
        async with httpx.AsyncClient(timeout=10) as client:
            res_138 = await client.get(url_d_138, headers=headers_lib())
        
        # å°è¯•å¤šç§æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ¨¡å¼
        patterns = [
            r'<span class="date">(.*?)</span><a href="/(.*?)/" target="_blank">(.*?)</a>',
            r'<a[^>]*href="[^"]*"[^>]*>([\w\.-]+\.[a-zA-Z]{2,})</a>',
            r'([\w\.-]+\.[a-zA-Z]{2,})'
        ]
        
        domains = []
        for pattern in patterns:
            matches = re.findall(pattern, res_138.text, re.S)
            if matches:
                if isinstance(matches[0], tuple):
                    domains.extend([match[-1] for match in matches])
                else:
                    domains.extend(matches)
                break
        
        domains = list(set([d.strip() for d in domains if d.strip() and '.' in d]))
        
        if domains:
            print(f"[âœ“] ip138 æˆåŠŸåæŸ¥åˆ° {len(domains)} ä¸ªåŸŸå: {ip}")
            return ip, domains
    except Exception as e:
        print(f"[!] ip138 åæŸ¥å¤±è´¥: {ip} - {e}")

    print(f"[!] æ‰€æœ‰åæŸ¥æ–¹æ³•å‡å¤±è´¥: {ip}")
    return ip, []

# å¼‚æ­¥æ‰§è¡Œå‘½ä»¤
async def run_cmd_async(cmd):
    if DEBUG_FSCAN:
        print(f"[cmd] å¼‚æ­¥æ‰§è¡Œå‘½ä»¤: {cmd}")
    proc = await asyncio.create_subprocess_shell(
        cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await proc.communicate()
    stdout_str = stdout.decode(errors='ignore').strip()
    stderr_str = stderr.decode(errors='ignore').strip()

    if proc.returncode != 0:
        print(f"[ERROR] å‘½ä»¤æ‰§è¡Œå¤±è´¥: {cmd}")
        print(f"[ERROR] è¿”å›ç : {proc.returncode}")
        print(f"[ERROR] stderr: {stderr_str}")
        return None, stderr_str  # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯é€€å‡º

    # await finalize_report_directory(report_path, root)

    return stdout_str, stderr_str
# ------------------------------------
# ç›®å½•åˆå§‹åŒ–
def init_dirs():
    for d in ["temp", "output"]:
        os.makedirs(d, exist_ok=True)

# è½½å…¥è¿‡æ»¤åŸŸå
def load_filter_domains(path):
    if os.path.exists(path):
        return {line.strip().lower() for line in open(path, encoding="utf-8") if line.strip()}
    return set()

# è½½å…¥CDN IPæ®µ
def load_cdn_ranges(path):
    ranges = []
    if os.path.exists(path):
        for line in open(path, encoding="utf-8"):
            line = line.strip()
            if line:
                try:
                    net = ipaddress.ip_network(line if '/' in line else line + '/32', strict=False)
                    ranges.append(net)
                except ValueError:
                    print(f"[!] æ— æ•ˆCDNæ¡ç›®: {line}")
    return ranges

# åˆ¤æ–­IPæ˜¯å¦å±äºCDN
def is_cdn_ip(ip, cdn_ranges):
    try:
        ip_obj = ipaddress.ip_address(ip)
        return any(ip_obj in net for net in cdn_ranges)
    except ValueError:
        return False

# ------------------------------------
# å¤šè¿›ç¨‹è§£æJSONå—ï¼Œå¢åŠ   ä¿¡æ¯æ”¶é›†
def parse_json_lines_chunk(lines_chunk, cdn_ranges, existing_cdn_dyn_ips, filter_domains, target_domain=None):
    domain_ip_map = defaultdict(set)
    url_title_list = []
    url_root_map = {}
    url_body_info_map = {}
    filtered_non_200_urls = []  # æ–°å¢ï¼Œç”¨äºä¿å­˜é200/301/302çš„urlå’ŒçŠ¶æ€ç 
    redirect_domains_set = set()  # æ–°å¢ï¼Œç”¨äºä¿å­˜è·³è½¬å‘ç°çš„åŸŸå
    body_fqdn_filtered_set = set()
    body_domains_filtered_set = set()
    # ä½¿ç”¨ç¯å¢ƒå˜é‡è·å–tlds.txtè·¯å¾„
    tlds_path = os.path.join(PROJECT_ROOT, "config/tlds.txt")
    
    tlds_content = None
    try:
        with open(tlds_path, "r", encoding="utf-8") as f:
            tlds_content = f.read()
    except FileNotFoundError:
        print(f"[!] è­¦å‘Š: æ— æ³•æ‰¾åˆ°{tlds_path}æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤TLDåˆ—è¡¨")
        tlds_content = "com\nnet\norg\nedu\ngov\nmil\ninfo\nbiz\nname\ncn\nuk\nde\nfr\njp\nkr\nau\nca\nru\nbr\nin\nit\nes\nnl\nse\nno\ndk\nfi\npl\nbe\nch\nat\ncz\nhu\npt\ngr\ntr\nil\nza\nmx\nsg\nhk\ntw\nmy\nth\nph\nvn\nid\n"
    
    VALID_TLDS = set(line.strip().lower() for line in tlds_content.strip().split('\n') if line.strip())
    seen_ips = set()
    for idx, line in enumerate(lines_chunk):
        try:
            item = json.loads(line)
            url = item.get("url", "").strip()
            final_url = item.get("final_url", "").strip()  # ä½¿ç”¨-follow-redirectsæ—¶çš„æœ€ç»ˆURL
            location_url = item.get("location", "").strip()  # ä¸ä½¿ç”¨-follow-redirectsæ—¶çš„è·³è½¬ä½ç½®
            
            # å¤„ç†è·³è½¬ä¿¡æ¯ï¼ˆæ”¯æŒä¸¤ç§æƒ…å†µï¼‰
            redirect_url = final_url if (final_url and final_url != url) else location_url
            
            # å¦‚æœå­˜åœ¨è·³è½¬ï¼Œè®°å½•è·³è½¬ä¿¡æ¯ç”¨äºåç»­èµ„äº§å‘ç°
            if redirect_url:
                try:
                    redirect_parsed = urlparse(redirect_url)
                    if redirect_parsed.hostname:
                        redirect_hostname = redirect_parsed.hostname.lower()
                        # æå–è·³è½¬åŸŸåçš„æ ¹åŸŸå
                        try:
                            redirect_root = get_fld(redirect_url, fix_protocol=False).lower()
                            # é¿å…è®°å½•ç›¸åŒçš„æ ¹åŸŸå
                            original_root = get_fld(url, fix_protocol=False).lower()
                            if redirect_root != original_root:
                                # åº”ç”¨ä¸body_domainsç›¸åŒçš„è¿‡æ»¤é€»è¾‘
                                if "cdn" not in redirect_root and "img" not in redirect_root:
                                    try:
                                        ext = tldextract.extract(redirect_root)
                                        if ext.domain and ext.suffix and ext.suffix.lower() in VALID_TLDS:
                                            filtered_redirect_root = f"{ext.domain}.{ext.suffix}".lower()
                                            # æ£€æŸ¥æ˜¯å¦å·²åœ¨è¿‡æ»¤åˆ—è¡¨ä¸­
                                            if (filtered_redirect_root not in filter_domains and 
                                                filtered_redirect_root not in new_filtered_domains and
                                                filtered_redirect_root not in redirect_domains_set):
                                                redirect_domains_set.add(filtered_redirect_root)
                                                new_filtered_domains.add(filtered_redirect_root)
                                                if DEBUG_FSCAN:
                                                    print(f"[+] å‘ç°è·³è½¬åŸŸå: {url} -> {redirect_url} (æ–°åŸŸå: {filtered_redirect_root})")
                                    except Exception:
                                        pass
                        except Exception:
                            # å¦‚æœæ— æ³•æå–æ ¹åŸŸåï¼Œä½¿ç”¨hostnameå¹¶åº”ç”¨è¿‡æ»¤
                            if "cdn" not in redirect_hostname and "img" not in redirect_hostname and "." in redirect_hostname:
                                try:
                                    ext = tldextract.extract(redirect_hostname)
                                    if ext.domain and ext.suffix and ext.suffix.lower() in VALID_TLDS:
                                        filtered_hostname = f"{ext.domain}.{ext.suffix}".lower()
                                        if (filtered_hostname not in filter_domains and 
                                            filtered_hostname not in new_filtered_domains and
                                            filtered_hostname not in redirect_domains_set):
                                            redirect_domains_set.add(filtered_hostname)
                                            new_filtered_domains.add(filtered_hostname)
                                            if DEBUG_FSCAN:
                                                print(f"[+] å‘ç°è·³è½¬åŸŸå: {url} -> {redirect_url} (æ–°åŸŸå: {filtered_hostname})")
                                except Exception:
                                    pass
                except Exception:
                    pass

            title = item.get("title", "").strip()
            tls_info = item.get("tls", {})  
            cert = tls_info.get("subject_cn", "").strip()
            ico = item.get("favicon_md5", "").strip()
            ico_mmh3 = item.get("favicon", "").strip()
            hash_info = item.get("hash", {})
            bd_hash = hash_info.get("body_md5", "").strip()
            bd_mmh3 = hash_info.get("body_mmh3", "").strip()
            a_ips = item.get("a", [])

            try:
                parsed_url = urlparse(url)
                hostname = parsed_url.hostname
                # åˆ¤æ–­æ˜¯å¦æ˜¯IP
                ipaddress.ip_address(hostname)
                root_domain = hostname  # ç›´æ¥ç”¨ IP
            except ValueError:
                try:
                    root_domain = get_fld(url, fix_protocol=False).lower()
                    # æ£€æŸ¥æ ¹åŸŸåæ˜¯å¦åœ¨è¿‡æ»¤åˆ—è¡¨ä¸­ï¼ˆä½†ä¸è¿‡æ»¤ç›®æ ‡åŸŸåæœ¬èº«ï¼‰
                    if target_domain and root_domain == target_domain.lower():
                        # ç›®æ ‡åŸŸåæœ¬èº«ä¸è¿‡æ»¤
                        pass
                    elif root_domain in filter_domains:
                        if DEBUG_FSCAN:
                            print(f"[!] {root_domain} åŸŸåè¢«è¿‡æ»¤äº† (é™æ€è¿‡æ»¤åˆ—è¡¨)")
                        continue
                    elif root_domain in new_filtered_domains:
                        if DEBUG_FSCAN:
                            print(f"[!] {root_domain} åŸŸåè¢«è¿‡æ»¤äº† (åŠ¨æ€è¿‡æ»¤åˆ—è¡¨)")
                        continue
                except Exception as e:
                    if DEBUG_FSCAN:
                        print(f"[!] æå–ä¸»åŸŸåå¤±è´¥: {url} é”™è¯¯: {e}")
                    continue
            url_root_map[url] = root_domain
            status_code = item.get("status_code")  # ç¡®è®¤å®é™…å­—æ®µ
            if status_code is None:
                status_code = 0  # æˆ–è€…é»˜è®¤ä¸€ä¸ªå€¼ï¼Œé˜²æ­¢æŠ¥é”™
            # ç‰¹æ®ŠçŠ¶æ€ç å•ç‹¬å¤„ç†
            if status_code in (403, 404):
                filtered_non_200_urls.append((url, status_code))
                continue  # è·³è¿‡æ­£å¸¸æµç¨‹ï¼Œä½†è®°å½•ç‰¹æ®ŠçŠ¶æ€ç 
            elif status_code not in (200, 301, 302):
                # å…¶ä»–éæ­£å¸¸çŠ¶æ€ç ä¹Ÿè®°å½•
                filtered_non_200_urls.append((url, status_code))
                continue  # è·³è¿‡åç»­æ­£å¸¸æµç¨‹
            url_title_list.append((url, title, cert, ico, bd_hash, tuple(sorted(a_ips)),ico_mmh3,bd_mmh3))

            for ip in a_ips:
                if is_cdn_ip(ip, cdn_ranges):
                    continue
                if ip in existing_cdn_dyn_ips:
                    continue
                if ip in seen_ips:
                    continue
                seen_ips.add(ip)
                domain_ip_map[root_domain].add(ip)
            body_fqdn_list = item.get("body_fqdn", [])
            body_domains_list = item.get("body_domains", [])

            filtered_fqdn = []
            for fqdn in body_fqdn_list:
                if fqdn  and "cdn" not in fqdn and "img" not in fqdn:
                    try:
                        ext = tldextract.extract(fqdn)
                        if ext.domain and ext.suffix and ext.suffix.lower() in VALID_TLDS:
                            root_domain = f"{ext.domain}.{ext.suffix}".lower()
                            if root_domain not in filter_domains and root_domain not in new_filtered_domains:
                                if is_domain_resolvable(root_domain):
                                    filtered_fqdn.append(fqdn.lower())
                                    new_filtered_domains.add(root_domain)

                    except Exception:
                        pass

            filtered_domains = []
            for domain in body_domains_list:
                if domain  and "cdn" not in domain and "img" not in domain:
                    try:
                        ext = tldextract.extract(domain)
                        if ext.domain and ext.suffix and ext.suffix.lower() in VALID_TLDS:
                            root_domain = f"{ext.domain}.{ext.suffix}".lower()
                            if root_domain not in filter_domains and root_domain not in new_filtered_domains:
                                if is_domain_resolvable(root_domain):
                                    filtered_domains.append(domain.lower())
                                    new_filtered_domains.add(root_domain)

                    except Exception:
                        pass

            # ç»Ÿè®¡è¿‡æ»¤æƒ…å†µ
            original_fqdn_count = len(body_fqdn_list)
            original_domains_count = len(body_domains_list)
            filtered_fqdn_count = len(filtered_fqdn)
            filtered_domains_count = len(filtered_domains)
            
            body_fqdn_filtered_set.update(set(body_fqdn_list) - set(filtered_fqdn))
            body_domains_filtered_set.update(set(body_domains_list) - set(filtered_domains))
            
            # ä¿å­˜ç»“æœ
            url_body_info_map[url] = {
                "body_fqdn": filtered_fqdn,
                "body_domains": filtered_domains
            }
            if new_filtered_domains:
                with open(DYNAMIC_FILTER_FILE, "a", encoding="utf-8") as f:
                    for dom in sorted(new_filtered_domains):
                        f.write(dom + "\n")

        except Exception as e:
            if DEBUG_FSCAN:
                print(f"[!] JSONè§£æå¼‚å¸¸ (ç¬¬ {idx} è¡Œ): {e}")
            continue

    return domain_ip_map, url_title_list, url_root_map, url_body_info_map, filtered_non_200_urls, redirect_domains_set

def chunked_iterable(iterable, size):
    """æŒ‰sizeåˆ‡åˆ†è¿­ä»£å™¨æˆå°å—"""
    it = iter(iterable)
    while True:
        chunk = list(islice(it, size))
        if not chunk:
            break
        yield chunk

# ------------------------------------
# å°è£…ï¼šç¡®ä¿ base_info æ–‡ä»¶å­˜åœ¨ï¼ˆå¦‚æ— åˆ™åæŸ¥å¹¶å†™å…¥ï¼‰
async def ensure_base_info(root, report_path, valid_ips, urls, titles, filter_domains, existing_cdn_dyn_ips, url_body_info_map, folder, redirect_domains=None):
    base_info_files = list(report_path.glob(f"base_info_{root}.txt"))

    if base_info_files:
        print(f"[i] base_info æ–‡ä»¶å­˜åœ¨ï¼Œè·³è¿‡å†™å…¥ base_info")
        return None  # å·²æœ‰æ–‡ä»¶ï¼Œä¸éœ€è¦åæŸ¥
    else:
        print(f"[i] base_info æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹åæŸ¥å¹¶å†™å…¥ base_info")
        ip_domain_map = await resolve_and_filter_domains(valid_ips, filter_domains, existing_cdn_dyn_ips, folder)
        print("[âœ“] å®ŒæˆåæŸ¥åŸŸå")
        print(ip_domain_map)
        await write_base_report(root, report_path, valid_ips, urls, titles, ip_domain_map, url_body_info_map, redirect_domains)
        return ip_domain_map
async def per_domain_flow_sync_async(root, ips, urls, titles, cdn_ranges, filter_domains, existing_cdn_dyn_ips, url_body_info_map, redirect_domains=None):
    print(f"\n[>] æ‰§è¡ŒåŸŸåæµç¨‹: {root}")
    folder = prepare_domain_folder(root)
    valid_ips = write_valid_ips(folder, ips, cdn_ranges, existing_cdn_dyn_ips)
    write_urls(folder, urls)
    mark_classification_complete(folder)

    # æŠ¥å‘Šç›®å½•è®¾ç½®
    base_report_root = Path("output")
    standard_dir = base_report_root / root
    finish_dir = base_report_root / f"{root}_finish"
    exp_dir = base_report_root / f"{root}_vul"

    if finish_dir.exists():
        print(f"[i] å‘ç°å·²æœ‰å®ŒæˆæŠ¥å‘Šç›®å½•: {finish_dir}")
        return  # å·²å®Œæˆï¼Œè·³è¿‡å¤„ç†
    elif exp_dir.exists():
        report_path = exp_dir
        print(f"[i] å‘ç°å·²æœ‰æ¼æ´æŠ¥å‘Šç›®å½•: {report_path}")
    elif standard_dir.exists():
        report_path = standard_dir
        print(f"[i] ä½¿ç”¨å·²æœ‰æ‰«æä¸­ç›®å½•: {report_path}")
    else:
        report_path = standard_dir
        report_path.mkdir(parents=True, exist_ok=True)
        print(f"[+] åˆ›å»ºæ–°æŠ¥å‘Šç›®å½•: {report_path}")

    # è·å–ç›®å½•ä¸‹å·²æœ‰æ–‡ä»¶
    files = list(report_path.iterdir())

    if not files:
        print(f"[+] æŠ¥å‘Šç›®å½•ä¸ºç©ºï¼Œå¼€å§‹æ­£å¸¸æ‰«æ")
        print(f"[*] æœ‰æ•ˆIPåˆ—è¡¨: {valid_ips}")
        print(f"å½“å‰åŸŸå: {root}")

        ip_domain_map,cdn_ip_to_remove = await resolve_and_filter_domains(valid_ips, filter_domains, existing_cdn_dyn_ips, folder)
        print("[âœ“] å®ŒæˆåæŸ¥åŸŸå")
        valid_ips = [ip for ip in valid_ips if ip not in cdn_ip_to_remove]

        await write_base_report(root, report_path, valid_ips, urls, titles, ip_domain_map, url_body_info_map, redirect_domains, filter_domains)
        await write_representative_urls(folder, titles, urls)
        if not ONLY_DOMAIN_MODE:
            await run_security_scans(root, folder, report_path)

    else:
        ip_domain_map = await ensure_base_info(
            root, report_path, valid_ips, urls, titles,
            filter_domains, existing_cdn_dyn_ips, url_body_info_map, folder, redirect_domains
        )

        base_info_files = list(report_path.glob(f"base_info_{root}.txt"))

        has_scan_done = any(f.name == "æ‰«æå®Œæˆ.txt" for f in files)
        if base_info_files and has_scan_done:
            print(f"[âœ“] ç›®æ ‡ {root} å·²å®Œæˆæ‰«æï¼ˆå­˜åœ¨ base_info å’Œ æ‰«æå®Œæˆ.txtï¼‰ï¼Œè·³è¿‡ã€‚")
            return

        elif base_info_files:
            print(f"[+] åªæœ‰ base_info æ–‡ä»¶ï¼Œå‡†å¤‡å¤„ç†")

            # æ— è®ºå¦‚ä½•éƒ½è¦å¤„ç†æ‰©å±•ç»“æœ
            await merge_all_expanded_results(str(report_path), root, redirect_domains, filter_domains)

            if ONLY_DOMAIN_MODE:
                print(f"[i] è·³è¿‡ run_security_scansï¼Œå› å¯ç”¨äº† --test")
                return

            await run_security_scans(root, folder, report_path)


def prepare_domain_folder(root):
    folder = Path("output") / root
    folder.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºå­ç›®å½•ç»“æ„
    input_folder = folder / "input"  # å­˜æ”¾æ‰«æè¾“å…¥æ–‡ä»¶
    input_folder.mkdir(exist_ok=True)
    
    print(f"[âœ“] åˆ›å»ºåŸŸåç›®å½•: {folder}")
    return folder
def natural_sort_key(s):
    # åˆ†å‰²å­—ç¬¦ä¸²ï¼Œæ•°å­—è½¬intï¼Œå­—æ¯å°å†™
    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\d+)', s)]

def write_valid_ips(folder, ips, cdn_ranges, existing_cdn_dyn_ips):
    valid_ips = []
    input_folder = folder / "input"
    input_folder.mkdir(exist_ok=True)

    # å…ˆè¯»å– all_a_records.txtï¼ˆå¦‚æœå­˜åœ¨ï¼‰é‡Œçš„å†å² IP
    all_a_records_path = input_folder / "all_a_records.txt"
    if all_a_records_path.exists():
        with open(all_a_records_path, "r") as f:
            existing_all_ips = set(line.strip() for line in f if line.strip())
    else:
        existing_all_ips = set()

    with open(input_folder / "a_records.txt", "w") as a, open(all_a_records_path, "a") as all_a:
        for ip in sorted(ips):
            if is_cdn_ip(ip, cdn_ranges) or ip in existing_cdn_dyn_ips:
                print(f"[-] CDNè·³è¿‡: {ip}")
                continue
            if ip in existing_all_ips:
                print(f"[!] å·²å­˜åœ¨äº all_a_records.txt ä¸­ï¼Œè·³è¿‡: {ip}")
                continue
            a.write(ip + "\n")
            all_a.write(ip + "\n")
            valid_ips.append(ip)

    return valid_ips


def write_urls(folder, urls):
    input_folder = folder / "input"
    input_folder.mkdir(exist_ok=True)
    with open(input_folder / "urls.txt", "w") as u:
        for url in urls:
            u.write(url + "\n")


def mark_classification_complete(folder):
    try:
        # finish.txt ä¿ç•™åœ¨æ ¹ç›®å½•ä½œä¸ºå®Œæˆæ ‡è®°
        with open(folder / "finish.txt", "w", encoding="utf-8") as f:
            f.write("åˆ†ç±»å®Œæˆ")
        print(f"[âœ“] æ ‡è®°åˆ†ç±»å®Œæˆ: {folder}/finish.txt")
    except Exception as e:
        print(f"[!] å†™å…¥ finish.txt å¤±è´¥: {e}")

def create_simplified_output(root, report_folder):
    """åˆ›å»ºç®€åŒ–çš„è¾“å‡ºç»“æ„ï¼Œåªä¿ç•™æ ¸å¿ƒæ–‡ä»¶"""
    core_folder = Path("output") / root
    core_folder.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ç®€åŒ–ç›®å½•ï¼ˆé¿å…é‡å¤å¤åˆ¶ï¼‰
    if str(report_folder.resolve()) == str(core_folder.resolve()):
        print(f"[i] å·²æ˜¯ç®€åŒ–è¾“å‡ºç›®å½•ï¼Œæ— éœ€å¤åˆ¶")
        return core_folder
    
    # åªå¤åˆ¶æ ¸å¿ƒæ–‡ä»¶åˆ°ç®€åŒ–ç›®å½•
    core_files = [
        f"base_info_{root}.txt",
        "finish.txt"
    ]
    
    for file_name in core_files:
        src_file = report_folder / file_name
        dst_file = core_folder / file_name
        if src_file.exists():
            shutil.copy2(src_file, dst_file)
            print(f"[âœ“] å¤åˆ¶æ ¸å¿ƒæ–‡ä»¶: {file_name}")
    
    # å¤åˆ¶æ‰«å±•æ•°æ®ç›®å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    tuozhan_src = report_folder / "tuozhan"
    tuozhan_dst = core_folder / "tuozhan"
    if tuozhan_src.exists():
        if tuozhan_dst.exists():
            shutil.rmtree(tuozhan_dst)
        shutil.copytree(tuozhan_src, tuozhan_dst)
        print(f"[âœ“] å¤åˆ¶æ‰©å±•æ•°æ®ç›®å½•: tuozhan")
    
    # å¤åˆ¶inputç›®å½•ï¼ˆåŒ…å«æ‰«æè¾“å…¥æ•°æ®å’ŒæŠ¥å‘Šæ–‡ä»¶ï¼‰
    input_src = report_folder / "input"
    input_dst = core_folder / "input"
    if input_src.exists():
        if input_dst.exists():
            shutil.rmtree(input_dst)
        shutil.copytree(input_src, input_dst)
        print(f"[âœ“] å¤åˆ¶è¾“å…¥æ•°æ®ç›®å½•: input")
    
    print(f"[âœ“] åˆ›å»ºç®€åŒ–è¾“å‡º: {core_folder}")
    return core_folder


def create_report_folder(root):
    report_folder = Path("output") / root
    report_folder.mkdir(parents=True, exist_ok=True)
    print(f"[âœ“] åˆ›å»ºæŠ¥å‘Šç›®å½•: {report_folder}")
    return report_folder

def update_a_records_after_scan(cdn_ip_to_remove, a_record_file):
    path = a_record_file / "input" / "a_records.txt"
    if not path.exists():
        print(f"[!] æœªæ‰¾åˆ°æ–‡ä»¶: {a_record_file}/input/a_records.txt")
        return

    with open(path, "r") as f:
        lines = f.readlines()

    new_lines = [line for line in lines if line.strip() not in cdn_ip_to_remove]

    with open(path, "w") as f:
        f.writelines(new_lines)

    print(f"[âœ“] å·²ä» a_records.txt ä¸­ç§»é™¤ {cdn_ip_to_remove} ")


CDN_KEYWORDS = [
    "cloudfront.net", "r.cloudfront.net",
    "cloudflare.com", "cloudflare.net",
    "akamai", "akamaiedge.net", "akamaized.net", "akamaitechnologies.com",
    "fastly.net", "fastlylb.net",
    "googleusercontent.com", ".gws",
    "dnsv1.com", "tcdn.qq.com",
    "baidubce.com",
    "alicdn.com", "aliyun.com",
    "wscdns.com", "wscloudcdn.com",
    "edgecastcdn.net",
    "cdnetworks.net", "cdngc.net",
    "incapdns.net", "impervadns.net"
]

def is_cdn_domain(domain: str) -> bool:
    return any(keyword in domain.lower() for keyword in CDN_KEYWORDS)
def is_cdn_ip_new(ip, domains):
    # print(f"[+] åˆ¤æ–­IP: {ip} æ˜¯å¦æ˜¯CDNèŠ‚ç‚¹")
    
    # æ¡ä»¶1ï¼šåŸŸåæ•°é‡è¿‡å¤šï¼Œç›´æ¥åˆ¤å®šä¸ºCDN
    if len(domains) > 45:
        print(f"[-] åŸŸåæ•°é‡å¤§äº45), ç›´æ¥åˆ¤å®šä¸ºCDN")
        return True

    # éšæœºé€‰ä¸€ä¸ªåŸŸååšæµ‹è¯•
    test_domain = random.choice(domains)
    # print(f"[+] é€‰å–çš„æµ‹è¯•åŸŸå: {test_domain}")

    try:
        # æ­£å‘è§£æï¼šåŸŸå -> IPåˆ—è¡¨
        ips = socket.gethostbyname_ex(test_domain)[2]
        # print(f"[+] æ­£å‘è§£æ {test_domain} å¾—åˆ°IPåˆ—è¡¨: {ips}")
        
        if ip not in ips:
            # print(f"[-] ç›®æ ‡IP {ip} ä¸åœ¨åŸŸåè§£æçš„IPåˆ—è¡¨ä¸­ï¼Œåˆ¤å®šä¸ºCDN")
            return True

        if len(ips) > 4:
            # print(f"[-] æ­£å‘è§£æIPåˆ—è¡¨æ•°é‡è¶…è¿‡4 ({len(ips)}), åˆ¤å®šä¸ºCDN")
            return True

    except Exception as e:
        # print(f"[-] è§£æå¼‚å¸¸: {e}ï¼Œåˆ¤å®šä¸ºCDN")
        return True

    print(f"[+] é€šè¿‡æ‰€æœ‰åˆ¤æ–­ {ip} éCDNèŠ‚ç‚¹")
    return False

async def resolve_and_filter_domains(valid_ips, filter_domains, existing_cdn_dyn_ips, folder):
    global reverse_lookup_semaphore
    if reverse_lookup_semaphore is None:
        reverse_lookup_semaphore = asyncio.Semaphore(3)  # é€‚å½“å¹¶å‘æ•°
    
    ip_domain_map = defaultdict(list)
    cdn_ip_to_remove = set()
    
    print(f"[*] å¼€å§‹åæŸ¥ {len(valid_ips)} ä¸ªIPåœ°å€...")
    print(f"[*] ä½¿ç”¨ 3 ä¸ªå¹¶å‘è¿æ¥ï¼Œæ¯ä¸ªIPæœ€å¤§è¶…æ—¶ 45ç§’")
    
    # ä½¿ç”¨å¼‚æ­¥å¹¶å‘å¤„ç†åæŸ¥
    successful_lookups = 0
    failed_lookups = 0
    
    async def process_ip(ip):
        nonlocal successful_lookups, failed_lookups
        async with reverse_lookup_semaphore:
            try:
                # ä¸ºæ¯ä¸ªIPè®¾ç½®æœ€å¤§45ç§’è¶…æ—¶
                result = await asyncio.wait_for(reverse_lookup_ip_async(ip), timeout=45)
                if result[1]:  # å¦‚æœæœ‰åŸŸåç»“æœ
                    successful_lookups += 1
                    print(f"[âœ“] è¿›åº¦: {successful_lookups + failed_lookups}/{len(valid_ips)} (æˆåŠŸ: {successful_lookups})")
                else:
                    failed_lookups += 1
                return result
            except asyncio.TimeoutError:
                failed_lookups += 1
                print(f"[!] IP {ip} åæŸ¥æ€»ä½“è¶…æ—¶(45ç§’)")
                return ip, []
            except Exception as e:
                failed_lookups += 1
                print(f"[!] IP {ip} åæŸ¥å¼‚å¸¸: {e}")
                return ip, []
    
    # åˆ†æ‰¹å¤„ç†IPï¼Œé¿å…ä¸€æ¬¡æ€§åˆ›å»ºå¤ªå¤šä»»åŠ¡
    batch_size = 10
    all_results = []
    
    for i in range(0, len(valid_ips), batch_size):
        batch_ips = valid_ips[i:i+batch_size]
        print(f"[*] å¤„ç†IPæ‰¹æ¬¡ {i//batch_size + 1}/{(len(valid_ips)-1)//batch_size + 1}: {len(batch_ips)} ä¸ªIP")
        
        tasks = [process_ip(ip) for ip in batch_ips]
        try:
            # æ¯æ‰¹æœ€å¤§8åˆ†é’Ÿè¶…æ—¶
            batch_results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True), 
                timeout=480  # 8åˆ†é’Ÿæ‰¹æ¬¡è¶…æ—¶
            )
            all_results.extend(batch_results)
        except asyncio.TimeoutError:
            print(f"[!] IPæ‰¹æ¬¡ {i//batch_size + 1} è¶…æ—¶ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")
            # ä¸ºè¶…æ—¶çš„IPæ·»åŠ ç©ºç»“æœ
            all_results.extend([(ip, []) for ip in batch_ips])
    
    results = all_results
    
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"[!] IP {valid_ips[i]} åæŸ¥å¤±è´¥: {result}")
            continue
        
        ip_, domains = result
        if not domains:
            print(f"[!] {ip_} åæŸ¥æ— ç»“æœ")
            continue

        if is_cdn_ip_new(ip_, domains):
            print(f"[!] {ip_} è¯†åˆ«ä¸ºCDN IPï¼Œç§»é™¤")
            cdn_ip_to_remove.add(ip_)
        else:
            ip_domain_map[ip_].extend(domains)

        is_cdn = False
        for d in domains:
            try:
                if isinstance(d, list):  # ä¿®å¤ç‚¹
                    d = d[0]
                domain_line = d.strip()
                match = re.search(r'([a-zA-Z0-9-]+\.)+[a-zA-Z]{2,}', domain_line)
                if not match:
                    continue

                domain = match.group(0)

                if is_cdn_domain(domain):
                    print(f"[!] CDN åŸŸå {domain}ï¼Œæ ‡è®° CDN IP: {ip_}")
                    cdn_ip_to_remove.add(ip_)
                    is_cdn = True
                    break

                # æå–ä¸»åŸŸå¹¶åˆ¤æ–­æ˜¯å¦è¢«è¿‡æ»¤
                ext = tldextract.extract(domain)
                root_domain = f"{ext.domain}.{ext.suffix}"
                if not any(fd in root_domain for fd in filter_domains):
                    ip_domain_map[ip_].append(domain)

            except Exception as e:
                if DEBUG_FSCAN:
                    print(f"[!] åŸŸåå­—ç¬¦ä¸²å¤„ç†å¼‚å¸¸: {e}")

        if is_cdn:
            continue  # é¿å…è®°å½•ä»»ä½•åŸŸå
    # âœ… å†™å…¥ CDN IP å¹¶æ›´æ–° a_records
    new_cdn_ips = cdn_ip_to_remove - existing_cdn_dyn_ips
    if new_cdn_ips:
        with open(CDN_DYNAMIC_PATH, "a", encoding="utf-8") as f:
            for ip in new_cdn_ips:
                f.write(f"{ip}\n")
        existing_cdn_dyn_ips.update(new_cdn_ips)
        update_a_records_after_scan(cdn_ip_to_remove, folder)

    # è¾“å‡ºåæŸ¥æ€»ç»“
    total_domains_found = sum(len(domains) for domains in ip_domain_map.values())
    print(f"\n[âœ“] IPåæŸ¥å®Œæˆ!")
    print(f"    - æ€»IPæ•°: {len(valid_ips)}")
    print(f"    - æˆåŠŸåæŸ¥: {successful_lookups}")
    print(f"    - å¤±è´¥/æ— ç»“æœ: {failed_lookups}")
    print(f"    - å‘ç°åŸŸåæ€»æ•°: {total_domains_found}")
    print(f"    - è¯†åˆ«CDN IP: {len(cdn_ip_to_remove)}")

    return ip_domain_map, cdn_ip_to_remove

def extract_root_domain(domain):
    ext = tldextract.extract(domain)
    if ext.suffix:
        return f"{ext.domain}.{ext.suffix}"
    return domain
async def query_platform_by_hash(hash_value, platform="fofa", hash_type="icon_hash", size=100, proxies=None):
    """
    é€šç”¨ hash/title æŸ¥è¯¢æ¥å£ï¼Œæ”¯æŒ FOFA / Hunterï¼Œè¿”å›åŸŸååˆ—è¡¨ã€‚
    :param hash_value: hash å€¼ï¼ˆicon_hash / body_hashï¼‰æˆ–æ ‡é¢˜å†…å®¹
    :param platform: å¹³å°æ ‡è¯† "fofa" / "hunter"
    :param hash_type: æŸ¥è¯¢ç±»å‹ icon_hash / body_hash / cert / title (FOFA) æˆ– web.icon / web.title (Hunter)
    :param size: æœ€å¤§è¿”å›æ•°é‡ï¼ˆfofa ç”¨ï¼Œhunter å›ºå®šä¸€é¡µ 100ï¼‰
    :param proxies: ä»£ç† URL å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "socks5h://127.0.0.1:7891" æˆ– "http://127.0.0.1:7890"
    """
    assert platform in {"fofa", "hunter"}, "platform å¿…é¡»æ˜¯ 'fofa' æˆ– 'hunter'"

    if platform == "fofa":
        query = f'{hash_type}="{hash_value}"'
        qbase64 = base64.b64encode(query.encode()).decode()
        url = (
            f"https://fofa.info/api/v1/search/all?"
            f"email={TEST_EMAIL}&key={TEST_KEY}&qbase64={qbase64}"
            f"&size={size}&fields=host"
        )

        try:
            async with httpx.AsyncClient(timeout=10) as client:
                r = await client.get(url)
                r.raise_for_status()
                data = r.json()

                if data.get("error") is False:
                    results = data.get("results", [])
                    if not results:
                        # print(f"[!] FOFA ç©ºç»“æœ: {hash_type}={hash_value}")
                        return []
                    first_item = results[0]
                    if isinstance(first_item, list):
                        return list(set(row[0] for row in results if row))
                    elif isinstance(first_item, str):
                        return list(set(results))
                    else:
                        print(f"[!] FOFA æœªçŸ¥ç»“æœæ ¼å¼: {type(first_item)}")
                        return []
                else:
                    print(f"[!] FOFA é”™è¯¯: {data.get('errmsg')}")
                    return []

        except Exception as e:
            print(f"[!] æŸ¥è¯¢å¤±è´¥ (fofa): {e}")
            return []

    else:  # Hunter æŸ¥è¯¢
        if hash_type == "title":
            query = f'web.title="{hash_value}"'
        else:
            query = f'web.icon="{hash_value}"'
        start_time = time.strftime("%Y-%m-%d", time.localtime(time.time() - 30*24*3600))
        end_time = time.strftime("%Y-%m-%d", time.localtime())
        url = (
            f"https://hunter.qianxin.com/openApi/search?"
            f"api-key={HUNTER_API_KEY}&search={query}&start_time={start_time}&end_time={end_time}"
            f"&page=1&page_size=100&is_web=3"
        )

        try:
            async with httpx.AsyncClient(timeout=10, proxy=proxies) as client:
                r = await client.get(url)
                r.raise_for_status()
                data = r.json()

                if data.get("code") != 200:
                    print(f"[!] Hunter é”™è¯¯: {data.get('message')}")
                    return []

                results = data.get("data", {}).get("arr", [])
                return list({r.get("domain") for r in results if r.get("domain")})

        except Exception as e:
            print(f"[!] æŸ¥è¯¢å¤±è´¥ (hunter): {e}")
            return []
def is_ip(string):
    """æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä¸ºIPåœ°å€ï¼ˆæ”¯æŒå¸¦ç«¯å£çš„æ ¼å¼ï¼‰"""
    # ç§»é™¤ç«¯å£éƒ¨åˆ†
    if ':' in string:
        string = string.split(':')[0]
    return re.match(r"^\d{1,3}(\.\d{1,3}){3}$", string) is not None
def clean_line(line):
    return line.strip().strip('"').strip("'").lower()

async def read_lines_from_file(filepath):
    lines = set()
    if os.path.exists(filepath):
        async with aiofiles.open(filepath, mode='r') as f:
            async for line in f:
                line = clean_line(line)
                if line:
                    lines.add(line)
    return lines
async def write_lines_to_file(filepath, lines):
    if not lines:
        return
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    async with aiofiles.open(filepath, mode='a') as f:
        for line in sorted(lines):
            await f.write(line + '\n')
def parse_url(line):
    line = clean_line(line)
    if not line:
        return None, None
    if line.startswith('http://') or line.startswith('https://'):
        parsed = urlparse(line)
        hostname = parsed.hostname
        return line, hostname
    else:
        return None, line  # treat as domain or IP
def strip_url_scheme(url: str) -> str:
    """å»æ‰ http:// æˆ– https://ï¼Œåªè¿”å› host"""
    url = url.strip()
    if url.startswith("http://") or url.startswith("https://"):
        parsed = urlparse(url)
        return parsed.hostname or url  # fallback
    return url

async def merge_all_expanded_results(report_folder: str, root_domain: str, redirect_domains: set = None, filter_domains: set = None):
    if filter_domains is None:
        filter_domains = set()
    tuozhan_path = os.path.join(report_folder, "tuozhan")
    all_dir = os.path.join(tuozhan_path, "all_tuozhan")
    os.makedirs(all_dir, exist_ok=True)

    existing_report_folder = f"./output/{root_domain}"
    existing_urls_raw = await read_lines_from_file(os.path.join(existing_report_folder, "input/urls.txt"))
    existing_urls_hosts = {strip_url_scheme(u) for u in existing_urls_raw}

    a_record_path = f"{existing_report_folder}/input/a_records.txt"
    existing_ips = await read_lines_from_file(a_record_path)

    # ä¿å­˜æ¥æºæ˜ å°„: {è¯¦ç»†æ¥æº: set(åŸŸå/IP)}
    source_host_map = defaultdict(set)

    # âœ… 1. å¤„ç† fofa å­ç›®å½•ä¸‹æ‰€æœ‰ txt æ–‡ä»¶
    for subfolder in ["fofa"]:
        full_path = os.path.join(tuozhan_path, subfolder)
        if not os.path.exists(full_path):
            continue

        for fname in os.listdir(full_path):
            if not fname.endswith(".txt"):
                continue

            file_path = os.path.join(full_path, fname)
            current_source = None
            domains = []

            async with aiofiles.open(file_path, mode='r') as f:
                async for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    if line.startswith("# æ¥æº:"):
                        original_source = line.replace("# æ¥æº:", "").strip()
                        # æ„å»ºè¯¦ç»†æ¥æº: "fofaçš„cert_vtmarkets.com.txt -> https://go.vtmarkets.com"
                        current_source = f"fofaçš„{fname} -> {original_source}"
                        continue
                    domain = clean_line(line)
                    if not domain:
                        continue
                    host = strip_url_scheme(domain)
                    if not host:
                        continue

                    if is_ip(host):
                        if host not in existing_ips:
                            source_host_map[current_source].add(host)
                    else:
                        if host not in existing_urls_hosts:
                            source_host_map[current_source].add(host)

    # âœ… 2. åˆå¹¶ root domains
    merged_roots = set()
    ip_re_path = os.path.join(tuozhan_path, "ip_re", "ip_domain_summary.txt")
    if os.path.exists(ip_re_path):
        async with aiofiles.open(ip_re_path, mode='r') as f:
            async for line in f:
                domain = clean_line(line)
                if not domain or is_ip(domain):
                    continue
                root = extract_root_domain(domain)
                if root and root not in existing_urls_hosts and root != root_domain:
                    merged_roots.add(root)

    # ğŸ†• æ·»åŠ ä»è·³è½¬å‘ç°çš„åŸŸåï¼ˆå·²ç»åœ¨parse_json_lines_chunkä¸­è¿‡æ»¤è¿‡ï¼‰
    if redirect_domains:
        redirect_count = 0
        for redirect_domain in redirect_domains:
            if redirect_domain and redirect_domain not in existing_urls_hosts:
                # éªŒè¯åŸŸåæ ¼å¼å¹¶æ’é™¤ä¸ä¸»åŸŸåç›¸åŒçš„åŸŸå
                if (not is_ip(redirect_domain) and '.' in redirect_domain and 
                    redirect_domain != root_domain):
                    # å†æ¬¡æ£€æŸ¥è¿‡æ»¤åˆ—è¡¨ï¼ˆåŒé‡ä¿æŠ¤ï¼‰
                    if redirect_domain not in filter_domains:
                        merged_roots.add(redirect_domain)
                        redirect_count += 1
        if redirect_count > 0:
            print(f"[+] ä»URLè·³è½¬å‘ç° {redirect_count} ä¸ªæ–°æ ¹åŸŸåï¼ˆå·²è¿‡æ»¤å¹¶æ’é™¤é‡å¤ï¼‰")

    # âœ… 3. é‡æ–°è®¾è®¡æ–‡ä»¶è¾“å‡ºæ ¼å¼ - æ‰€æœ‰æ–‡ä»¶éƒ½åŒ…å«æ¥æºä¿¡æ¯
    merged_ips_with_source = []  # [(ip, source), ...]
    merged_urls_with_source = []  # [(url, source), ...]
    merged_roots_with_source = []  # [(root_domain, source), ...]
    
    # æ·»åŠ è·³è½¬å‘ç°çš„æ ¹åŸŸåï¼ˆå¸¦æ¥æºæ ‡è¯†ï¼Œæ’é™¤ä¸»åŸŸåï¼‰
    if merged_roots:
        for root in merged_roots:
            if root != root_domain:
                merged_roots_with_source.append((root, "URLè·³è½¬å‘ç°"))
    
    for source, hosts in source_host_map.items():
        for host in hosts:
            if is_ip(host):
                merged_ips_with_source.append((host, source))
            else:
                # åˆ¤æ–­æ˜¯å¦ä¸ºä¸»åŸŸå
                root = extract_root_domain(host)
                if root and root == host:
                    # æ˜¯ä¸»åŸŸåï¼Œæ·»åŠ åˆ°root_domainsï¼ˆæ’é™¤ä¸å½“å‰æ‰«æä¸»åŸŸåé‡å¤çš„ï¼‰
                    if host != root_domain:
                        merged_roots_with_source.append((host, source))
                else:
                    # æ˜¯å­åŸŸåï¼Œæ·»åŠ åˆ°urls
                    merged_urls_with_source.append((host, source))
    
    # å†™å…¥ ip.txt - åªå­˜IPä½†æ ‡è¯†æ¥æº
    ip_txt_path = os.path.join(all_dir, "ip.txt")
    async with aiofiles.open(ip_txt_path, "w") as f:
        if merged_ips_with_source:
            current_source = None
            for ip, source in sorted(merged_ips_with_source, key=lambda x: (x[1], x[0])):
                if current_source != source:
                    current_source = source
                    await f.write(f"# æ¥æº: {source}\n")
                await f.write(f"{ip}\n")
        else:
            await f.write("# æš‚æ— IPç›®æ ‡\n")
    
    # å†™å…¥ urls.txt - åªå­˜å­åŸŸå/URLä½†æ ‡è¯†æ¥æº
    urls_txt_path = os.path.join(all_dir, "urls.txt")
    async with aiofiles.open(urls_txt_path, "w") as f:
        if merged_urls_with_source:
            current_source = None
            for url, source in sorted(merged_urls_with_source, key=lambda x: (x[1], x[0])):
                if current_source != source:
                    current_source = source
                    await f.write(f"# æ¥æº: {source}\n")
                await f.write(f"{url}\n")
        else:
            await f.write("# æš‚æ— URLç›®æ ‡\n")
    
    # å†™å…¥ root_domains.txt - æ‰€æœ‰ä¸»åŸŸåä½†æ ‡è¯†æ¥æº
    root_domains_path = os.path.join(all_dir, "root_domains.txt")
    async with aiofiles.open(root_domains_path, "w") as f:
        if merged_roots_with_source:
            current_source = None
            for root, source in sorted(merged_roots_with_source, key=lambda x: (x[1], x[0])):
                if current_source != source:
                    current_source = source
                    await f.write(f"# æ¥æº: {source}\n")
                await f.write(f"{root}\n")
        else:
            await f.write("# æš‚æ— æ ¹åŸŸåç›®æ ‡\n")

async def load_fofa_query_blacklist() -> set[str]:
    try:
        async with aiofiles.open(BLACKLIST_FILE_PATH, mode='r') as f:
            content = await f.read()
        return set(line.strip() for line in content.splitlines() if line.strip())
    except FileNotFoundError:
        return set()

async def save_fofa_query_blacklist(blacklist: set[str]):
    os.makedirs(os.path.dirname(BLACKLIST_FILE_PATH), exist_ok=True)
    async with aiofiles.open(BLACKLIST_FILE_PATH, mode='w') as f:
        for item in sorted(blacklist):
            await f.write(f"{item}\n")

async def write_expanded_reports(report_folder, ico_mmh3_set=None, body_mmh3_set=None, domain_list=None, use_hunter=False, hunter_proxies=None, hunter_ico_md5_list=None, cert_root_domains=None, cert_root_domain_map=None, ico_md5_url_map=None, ico_mmh3_url_map=None, body_md5_url_map=None, body_mmh3_url_map=None, title_set=None, title_url_map=None, enable_fofa: bool = True):

    tuozhan_dir = Path(report_folder) / "tuozhan"
    fofa_dir = tuozhan_dir / "fofa"
    ip_re_dir = tuozhan_dir / "ip_re"
    all_tuozhan_dir = tuozhan_dir / "all_tuozhan"
    tuozhan_dir.mkdir(parents=True, exist_ok=True)
    fofa_dir.mkdir(parents=True, exist_ok=True)
    ip_re_dir.mkdir(parents=True, exist_ok=True)
    all_tuozhan_dir.mkdir(parents=True, exist_ok=True)
    updated_blacklist = set()
    fofa_blacklist = await load_fofa_query_blacklist()

    if use_hunter:
        hunter_dir = tuozhan_dir / "hunter"
        hunter_dir.mkdir(parents=True, exist_ok=True)

    if ico_mmh3_set:
        for hash_value in sorted(ico_mmh3_set):
            if use_hunter:
                if not hunter_ico_md5_list:
                    print(f"[!] Hunter æŸ¥è¯¢éœ€è¦ä¼ å…¥ ico md5 åˆ—è¡¨ï¼Œå½“å‰ä¸ºç©ºï¼Œè·³è¿‡ icon_hash={hash_value}")
                    continue
                for md5_hash in hunter_ico_md5_list:
                    print(f"[+] æŸ¥è¯¢ HUNTER icon md5={md5_hash}")
                    try:
                        domains = await query_platform_by_hash(
                            md5_hash,
                            platform="hunter",
                            hash_type="icon_md5",
                            proxies=hunter_proxies
                        )
                        updated_blacklist.add(md5_hash)
                    except Exception as e:
                        print(f"[!] Hunter æŸ¥è¯¢å¤±è´¥: {e}")
                        continue
                    if not domains:
                        continue
                    file_path = hunter_dir / f"icon_md5_hunter_{md5_hash}.txt"
                    with open(file_path, "w", encoding="utf-8") as f:
                        if ico_md5_url_map and md5_hash in ico_md5_url_map:
                            for src in sorted(ico_md5_url_map[md5_hash]):
                                f.write(f"# æ¥æº: {src}\n")
                        for domain in domains:
                            f.write(f"{domain}\n")
            else:
                if enable_fofa:
                    if hash_value in fofa_blacklist:
                        print(f"[!] è·³è¿‡ FOFA æŸ¥è¯¢ (é»‘åå•): icon_hash={hash_value}")
                        continue
                    print(f"[+] æŸ¥è¯¢ FOFA icon_hash={hash_value}")
                    try:
                        domains = await query_platform_by_hash(
                            hash_value,
                            platform="fofa",
                            hash_type="icon_hash"
                        )
                        updated_blacklist.add(hash_value)
                    except Exception as e:
                        print(f"[!] FOFA æŸ¥è¯¢å¤±è´¥: {e}")
                        continue
                    if not domains:
                        continue
                    file_path = fofa_dir / f"icon_hash_{hash_value}.txt"
                    with open(file_path, "w", encoding="utf-8") as f:
                        if ico_mmh3_url_map and hash_value in ico_mmh3_url_map:
                            for src in sorted(ico_mmh3_url_map[hash_value]):
                                f.write(f"# æ¥æº: {src}\n")
                        for domain in domains:
                            f.write(f"{domain}\n")

    if body_mmh3_set and enable_fofa:
        for hash_value in sorted(body_mmh3_set):
            if hash_value in fofa_blacklist:
                print(f"[!] è·³è¿‡ FOFA æŸ¥è¯¢ (é»‘åå•): body_hash={hash_value}")
                continue
            print(f"[+] æŸ¥è¯¢ FOFA body_hash={hash_value}")
            try:
                domains = await query_platform_by_hash(
                    hash_value,
                    platform="fofa",
                    hash_type="body_hash"
                )
                updated_blacklist.add(hash_value)
            except Exception as e:
                print(f"[!] FOFA æŸ¥è¯¢å¤±è´¥: {e}")
                continue
            if not domains:
                continue
            file_path = fofa_dir / f"body_hash_{hash_value}.txt"
            with open(file_path, "w", encoding="utf-8") as f:
                if body_mmh3_url_map and hash_value in body_mmh3_url_map:
                    for src in sorted(body_mmh3_url_map[hash_value]):
                        f.write(f"# æ¥æº: {src}\n")
                for domain in domains:
                    f.write(f"{domain}\n")

    if cert_root_domains and enable_fofa:
        for domain in sorted(cert_root_domains):
            if domain in fofa_blacklist:
                print(f"[!] è·³è¿‡ FOFA æŸ¥è¯¢ (é»‘åå•): cert={domain}")
                continue
            print(f"[+] æŸ¥è¯¢ FOFA cert={domain}")
            try:
                domains = await query_platform_by_hash(
                    domain,
                    platform="fofa",
                    hash_type="cert"
                )
                updated_blacklist.add(domain)
            except Exception as e:
                print(f"[!] FOFA æŸ¥è¯¢å¤±è´¥: cert={domain} é”™è¯¯: {e}")
                continue
            if not domains:
                continue
            file_path = fofa_dir / f"cert_{domain}.txt"
            with open(file_path, "w", encoding="utf-8") as f:
                if cert_root_domain_map and domain in cert_root_domain_map:
                    for src in sorted(cert_root_domain_map[domain]):
                        f.write(f"# æ¥æº: {src}\n")
                for d in domains:
                    f.write(f"{d}\n")

    # æ·»åŠ æ ‡é¢˜æœç´¢åŠŸèƒ½
    if title_set and enable_fofa:
        for title in sorted(title_set):
            if title in fofa_blacklist:
                print(f"[!] è·³è¿‡ FOFA æŸ¥è¯¢ (é»‘åå•): title={title}")
                continue
            print(f"[+] æŸ¥è¯¢ FOFA title={title}")
            try:
                domains = await query_platform_by_hash(
                    title,
                    platform="fofa",
                    hash_type="title"
                )
                updated_blacklist.add(title)
            except Exception as e:
                print(f"[!] FOFA æŸ¥è¯¢å¤±è´¥: title={title} é”™è¯¯: {e}")
                continue
            if not domains:
                continue
            file_path = fofa_dir / f"title_{title[:50]}.txt"  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
            with open(file_path, "w", encoding="utf-8") as f:
                if title_url_map and title in title_url_map:
                    for src in sorted(title_url_map[title]):
                        f.write(f"# æ¥æº: {src}\n")
                for domain in domains:
                    f.write(f"{domain}\n")

    # Hunter æ ‡é¢˜æœç´¢
    if use_hunter and title_set:
        hunter_dir = tuozhan_dir / "hunter"
        hunter_dir.mkdir(parents=True, exist_ok=True)
        
        for title in sorted(title_set):
            print(f"[+] æŸ¥è¯¢ HUNTER title={title}")
            try:
                domains = await query_platform_by_hash(
                    title,
                    platform="hunter",
                    hash_type="title",
                    proxies=hunter_proxies
                )
                updated_blacklist.add(title)
            except Exception as e:
                print(f"[!] Hunter æŸ¥è¯¢å¤±è´¥: title={title} é”™è¯¯: {e}")
                continue
            if not domains:
                continue
            file_path = hunter_dir / f"title_hunter_{title[:50]}.txt"
            with open(file_path, "w", encoding="utf-8") as f:
                if title_url_map and title in title_url_map:
                    for src in sorted(title_url_map[title]):
                        f.write(f"# æ¥æº: {src}\n")
                for domain in domains:
                    f.write(f"{domain}\n")

    if domain_list:
        root_domains = {extract_root_domain(d) for d in domain_list if d}
        if root_domains:
            out_file = ip_re_dir / "ip_domain_summary.txt"
            with open(out_file, "w", encoding="utf-8") as f:
                for domain in sorted(root_domains):
                    f.write(f"{domain}\n")

    print("[+] å®ŒæˆæŸ¥è¯¢,å¼€å§‹æ±‡æ€»å†™å…¥æ–‡ä»¶")
    await save_fofa_query_blacklist(fofa_blacklist.union(updated_blacklist))



async def write_base_report(root: str, report_folder: Path, valid_ips: set[str], urls: list[str], titles: dict, ip_domain_map: dict[str, list[str]], url_body_info_map: dict[str, dict], redirect_domains: set = None, filter_domains: set = None):

    all_icos = set()
    all_body_hashes = set()
    all_certs = set()
    all_icos_mmh3 = set()
    all_body_mmh3 = set()
    all_reverse_domains = []
    all_titles = set()

    ico_md5_url_map = defaultdict(set)
    ico_mmh3_url_map = defaultdict(set)
    body_md5_url_map = defaultdict(set)
    body_mmh3_url_map = defaultdict(set)
    cert_root_url_map = defaultdict(set)
    title_url_map = defaultdict(set)

    repeat_map = defaultdict(list)

    indent1 = "  "
    indent2 = "    "

    out_path = report_folder / f"base_info_{root}.txt"
    with open(out_path, "w", encoding="utf-8") as out:
        out.write(f"{'='*30}\n[åŸºç¡€ä¿¡æ¯æ±‡æ€»] åŸŸå: {root}\n{'='*30}\n")

        # === 1. å…³è”IP ===
        out.write("å…³è”çœŸå®IP:\n")
        for ip in sorted(valid_ips):
            out.write(f"{indent1}- {ip}\n")

        # === 2. URL æ ‡é¢˜ä¿¡æ¯ & hash åˆ†ç±» ===
        out.write("\nURLå’Œæ ‡é¢˜:\n")
        for url in urls:
            title, cert, ico, body_hash, url_ips, ico_mmh3, bd_mmh3 = titles.get(url, ("", "", "", "", (), "", ""))
            # æ”¹è¿›é‡å¤æ£€æµ‹é€»è¾‘ï¼šä¸»è¦åŸºäºbody_hashå’Œtitleï¼Œå‡å°‘è¿‡åº¦ç»†åˆ†
            # å¦‚æœtitleä¸ºç©ºæˆ–æ˜¯é€šç”¨é”™è¯¯é¡µé¢ï¼Œåˆ™ä¸»è¦ç”¨body_hash
            if not title or title in black_titles or title in ["403 Forbidden", "404 Not Found", "", "301 Moved Permanently"]:
                key = (bd_mmh3, body_hash)  # ä¸»è¦åŸºäºå†…å®¹hash
            else:
                key = (title, bd_mmh3)  # åŸºäºæ ‡é¢˜å’Œå†…å®¹hash
            repeat_map[key].append((url, title, cert, ico, body_hash, ico_mmh3, bd_mmh3))

        for url_list in repeat_map.values():
            for i, (url, title, cert, ico, body_hash, ico_mmh3, bd_mmh3) in enumerate(url_list):
                if i > 0:
                    continue
                out.write(f"{indent1}- {url} [{title}]\n")
                if ico:
                    all_icos.add(ico)
                    ico_md5_url_map[ico].add(url)
                if body_hash:
                    all_body_hashes.add(body_hash)
                    body_md5_url_map[body_hash].add(url)
                if ico_mmh3:
                    all_icos_mmh3.add(ico_mmh3)
                    ico_mmh3_url_map[ico_mmh3].add(url)
                if bd_mmh3:
                    all_body_mmh3.add(bd_mmh3)
                    body_mmh3_url_map[bd_mmh3].add(url)
                if cert and cert.strip():
                    root_domain = extract_root_domain(cert.strip("*."))  # <-- éœ€ç¡®ä¿æ­¤å‡½æ•°å­˜åœ¨
                    if root_domain:
                        all_certs.add(cert)
                        cert_root_url_map[root_domain].add(url)
                if title and title.strip() and title not in black_titles:
                    all_titles.add(title.strip())
                    title_url_map[title.strip()].add(url)

        # === 3. IPåæŸ¥åŸŸå ===
        out.write("\nIPåæŸ¥åŸŸå:\n")
        for ip in sorted(valid_ips):
            if ip in ip_domain_map:
                out.write(f"{indent1}[IP] {ip}\n")
                for domain in ip_domain_map[ip]:
                    all_reverse_domains.append(domain)
                    out.write(f"{indent2}- {domain}\n")

        # === 4. URL body info ä¸­æŠ½å–çš„åŸŸå ===
        urls_for_root = [url for url in urls if url_body_info_map.get(url)]
        if urls_for_root:
            out.write(f"\n[URL BODY INFO - åŸŸå(ç›®å‰éœ€è¦æ‰‹åŠ¨ç­›é€‰): {root}]\n")
            url_domains_seen = {urlparse(url).hostname for url in urls_for_root if urlparse(url).hostname}
            domain_source_map = defaultdict(set)
            for url in urls_for_root:
                info = url_body_info_map.get(url, {})
                for d in info.get("body_fqdn", []) + info.get("body_domains", []):
                    if d not in url_domains_seen:
                        domain_source_map[d].add(url)

            for domain, source_urls in domain_source_map.items():
                if len(source_urls) == 1:
                    out.write(f"{indent1}{domain} [æ¥æº: {next(iter(source_urls))}]\n")
                else:
                    out.write(f"{indent1}{domain} [æ¥æºæ•°é‡: {len(source_urls)}]\n")

        # === 5. hash / cert æ±‡æ€» ===
        out.write(f"\n{'='*30}\nèµ„æºæ±‡æ€»:\n{'='*30}\n")
        out.write("\nè¯ä¹¦ä¸»åŸŸå:\n")
        for cert_domain in sorted(cert_root_url_map.keys()):
            out.write(f"{indent1}{cert_domain}\n")
        out.write("ico:\n")
        out.write(f"{indent1}md5:\n")
        for ico in sorted(all_icos):
            out.write(f"{indent2}{ico}\n")
        out.write(f"{indent1}mmh3_hash:\n")
        for h in sorted(all_icos_mmh3):
            out.write(f"{indent2}{h}\n")

        out.write("\nbody_hash:\n")
        out.write(f"{indent1}md5:\n")
        for h in sorted(all_body_hashes):
            out.write(f"{indent2}{h}\n")
        out.write(f"{indent1}mmh3_hash:\n")
        for h in sorted(all_body_mmh3):
            out.write(f"{indent2}{h}\n")



        out.write("\nasnä¿¡æ¯(æš‚æœªå®ç°):\n")

        # === 6. é‡å¤é¡µé¢èšç±» ===
        out.write(f"\n{'='*30}\né‡å¤ç½‘ç«™:\n{'='*30}\n\n")
        indent3 = indent2 * 2
        for key, url_infos in repeat_map.items():
            if len(url_infos) > 1:
                main_url, main_title, *_ = url_infos[0]
                out.write(f"{indent1}- é‡å¤äº: {main_url}  æ ‡é¢˜: {main_title}\n")
                for url, title, *_ in url_infos:
                    out.write(f"{indent2}- {url}\n")
                    out.write(f"{indent3}æ ‡é¢˜: {title}\n")

    # === 7. å†™å…¥æ‰©å±•æŸ¥è¯¢ç»“æœï¼ˆFOFA / hunterï¼‰===
    if all_reverse_domains or all_icos_mmh3 or all_body_mmh3 or cert_root_url_map or all_titles:
        await write_expanded_reports(
            report_folder=report_folder,
            ico_mmh3_set=all_icos_mmh3,
            body_mmh3_set=all_body_mmh3,
            domain_list=all_reverse_domains,
            use_hunter=False,
            hunter_proxies=None,
            hunter_ico_md5_list=all_icos,
            cert_root_domains=set(cert_root_url_map.keys()),
            cert_root_domain_map=cert_root_url_map,
            ico_md5_url_map=ico_md5_url_map,
            ico_mmh3_url_map=ico_mmh3_url_map,
            body_md5_url_map=body_md5_url_map,
            body_mmh3_url_map=body_mmh3_url_map,
            title_set=all_titles,
            title_url_map=title_url_map,
            enable_fofa=True

        )

    # === 8. æ±‡æ€» merge æŠ¥å‘Š ===ï¼ˆç§»åˆ°æ¡ä»¶å¤–ï¼Œç¡®ä¿æ€»æ˜¯æ‰§è¡Œï¼‰
    await merge_all_expanded_results(report_folder, root, redirect_domains, filter_domains)


async def write_representative_urls(folder, titles, urls):
    repeat_map = defaultdict(list)
    for url in urls:
        title, cert, ico, body_hash, url_ips, ico_mmh3, bd_mmh3 = titles.get(url, ("", "", "", "", (), "", ""))
        a_str = ",".join(sorted(url_ips))
        
        # å¦‚æœå…³é”®ä¿¡æ¯éƒ½ä¸ºç©ºï¼Œä½¿ç”¨URLæœ¬èº«ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼Œé¿å…è¯¯åˆ¤ä¸ºé‡å¤
        if not body_hash and not cert and not a_str and not ico:
            key = ("unique_url", url)  # æ¯ä¸ªURLéƒ½æœ‰å”¯ä¸€key
        else:
            key = (body_hash, cert, a_str, ico)
            
        repeat_map[key].append((url, title, cert, ico, body_hash, ico_mmh3, bd_mmh3))

    input_folder = folder / "input"
    input_folder.mkdir(exist_ok=True)
    path = input_folder / "representative_urls.txt"
    
    written_urls = []
    filtered_urls = []
    
    with open(path, "w", encoding="utf-8") as f:
        for url_list in repeat_map.values():
            if url_list:
                url, title, *_ = url_list[0]
                if title in black_titles:
                    filtered_urls.append((url, title))
                    continue
                f.write(url + "\n")
                written_urls.append((url, title))
    
    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆURLè¢«å†™å…¥ï¼Œé€‰æ‹©ä¸€äº›éé»‘åå•çš„URLæˆ–è‡³å°‘å†™å…¥ä¸€äº›URL
    if not written_urls:
        print(f"[!] æ‰€æœ‰URLè¢«æ ‡é¢˜è¿‡æ»¤ï¼Œå°è¯•å†™å…¥å¤‡ç”¨URL...")
        with open(path, "w", encoding="utf-8") as f:
            # å°è¯•æ‰¾åˆ°ä¸€äº›çŠ¶æ€ç ä¸º200çš„URL
            backup_urls = []
            for url in urls:
                title, cert, ico, body_hash, url_ips, ico_mmh3, bd_mmh3 = titles.get(url, ("", "", "", "", (), "", ""))
                # ä¼˜å…ˆé€‰æ‹©æœ‰å†…å®¹çš„URLï¼ˆéç©ºæ ‡é¢˜ä¸”ä¸æ˜¯å¸¸è§çš„é”™è¯¯é¡µé¢ï¼‰
                if title and title not in black_titles and ("200" in title or "login" in title.lower() or "portal" in title.lower()):
                    backup_urls.append((url, title))
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œè‡³å°‘å†™å…¥ä¸€äº›URL
            if not backup_urls:
                backup_urls = [(url, titles.get(url, ("",))[0]) for url in urls[:5]]  # å–å‰5ä¸ª
            
            for url, title in backup_urls[:10]:  # æœ€å¤šå†™å…¥10ä¸ª
                f.write(url + "\n")
            
            print(f"[+] å†™å…¥äº† {len(backup_urls[:10])} ä¸ªå¤‡ç”¨URLåˆ°representative_urls.txt")
    
    print(f"[+] representative_urls.txt: å†™å…¥ {len(written_urls)} ä¸ªURL, è¿‡æ»¤ {len(filtered_urls)} ä¸ªURL")


async def run_security_scans(root, folder, report_folder):
    afrog_report = report_folder / f"afrog_report_{root}.json"
    fscan_report = report_folder / f"fscan_result_{root}.txt"
    afrog_target_file = folder / "input" / "representative_urls.txt"
    fscan_target_file = folder / "input" / "a_records.txt"
    print(f"[*] æ£€æŸ¥afrogç›®æ ‡æ–‡ä»¶: {afrog_target_file}")
    if not afrog_target_file.exists():
        empty_file = report_folder / "afrogç›®æ ‡ä¸ºç©º.txt"
        empty_file.touch()  # åˆ›å»ºç©ºæ–‡ä»¶
        print(f"[!] afrogç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {afrog_target_file}ï¼Œå·²åˆ›å»º {empty_file}ï¼Œè·³è¿‡afrogæ‰«æ")
    elif os.path.getsize(afrog_target_file) == 0:
        empty_file = report_folder / "afrogç›®æ ‡ä¸ºç©º.txt"
        empty_file.touch()  # åˆ›å»ºç©ºæ–‡ä»¶
        print(f"[!] afrogç›®æ ‡æ–‡ä»¶ä¸ºç©º: {afrog_target_file}ï¼Œå·²åˆ›å»º {empty_file}ï¼Œè·³è¿‡afrogæ‰«æ")
    else:
        target_count = sum(1 for line in open(afrog_target_file, 'r') if line.strip())
        print(f"[+] afrogç›®æ ‡æ–‡ä»¶æœ‰æ•ˆï¼ŒåŒ…å« {target_count} ä¸ªURLç›®æ ‡")
        afrog_cmd = AFROG_CMD_TEMPLATE.format(target_file=str(afrog_target_file), output_file=str(afrog_report))
        print(f"[*] æ‰§è¡Œafrogæ‰«æå‘½ä»¤: {afrog_cmd}")
        result = await run_cmd_async(afrog_cmd)
        if result is None:
            print(f"[!] afrogæ‰«æå¤±è´¥ï¼Œè·³è¿‡")
            return
        else:
            print(f"[âœ“] afrogæ‰«æå®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜è‡³: {afrog_report}")

    print(f"[*] æ£€æŸ¥fscanç›®æ ‡æ–‡ä»¶: {fscan_target_file}")
    if not fscan_target_file.exists():
        empty_file = report_folder / "fscanç›®æ ‡ä¸ºç©º.txt"
        empty_file.touch()
        print(f"[!] fscanç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {fscan_target_file}ï¼Œå·²åˆ›å»º {empty_file}ï¼Œè·³è¿‡fscanæ‰«æ")
    elif os.path.getsize(fscan_target_file) == 0:
        empty_file = report_folder / "fscanç›®æ ‡ä¸ºç©º.txt"
        empty_file.touch()
        print(f"[!] fscanç›®æ ‡æ–‡ä»¶ä¸ºç©º: {fscan_target_file}ï¼Œå·²åˆ›å»º {empty_file}ï¼Œè·³è¿‡fscanæ‰«æ")
    else:
        target_count = sum(1 for line in open(fscan_target_file, 'r') if line.strip())
        print(f"[+] fscanç›®æ ‡æ–‡ä»¶æœ‰æ•ˆï¼ŒåŒ…å« {target_count} ä¸ªIPç›®æ ‡")
        fscan_cmd = FSCAN_CMD_TEMPLATE.format(target_file=str(fscan_target_file), output_file=str(fscan_report))
        print(f"[*] æ‰§è¡Œfscanæ‰«æå‘½ä»¤: {fscan_cmd}")
        result = await run_cmd_async(fscan_cmd)
        if result is None:
            print(f"[!] fscanæ‰«æå¤±è´¥ï¼Œè·³è¿‡")
            return
        else:
            print(f"[âœ“] fscanæ‰«æå®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜è‡³: {fscan_report}")
    await finalize_report_directory(report_folder, root)


async def finalize_report_directory(report_folder, root):
    afrog_report = report_folder / f"afrog_report_{root}.json"
    
    # æ£€æŸ¥afrogæŠ¥å‘Šæ˜¯å¦å­˜åœ¨ä¸”æœ‰æ¼æ´å†…å®¹
    has_vulns = False
    if afrog_report.exists():
        try:
            with open(afrog_report, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if content and content != "[]":  # ä¸æ˜¯ç©ºæ•°ç»„
                    has_vulns = True
                    print(f"[+] æ£€æµ‹åˆ°afrogæ¼æ´æŠ¥å‘Š: {afrog_report}")
        except Exception as e:
            print(f"[!] è¯»å–afrogæŠ¥å‘Šå¤±è´¥: {e}")
    
    # ä½¿ç”¨ç®€åŒ–è¾“å‡ºç»“æ„ï¼Œä¸å†é‡å‘½åå¤æ‚ç›®å½•
    print(f"[*] åˆ›å»ºç®€åŒ–è¾“å‡ºç»“æ„...")
    simplified_folder = create_simplified_output(root, report_folder)
    
    # å†™å…¥æ‰«æå®Œæˆæ ‡å¿—
    scan_done_path = simplified_folder / "finish.txt"
    scan_done_path.write_text("æ‰«æå·²å®Œæˆ", encoding="utf-8")
    
    # å¦‚æœå‘ç°æ¼æ´ï¼Œåœ¨æ–‡ä»¶åä¸­æ ‡è®°
    if has_vulns:
        vuln_marker = simplified_folder / "å‘ç°æ¼æ´.txt"
        vuln_marker.write_text("æ£€æµ‹åˆ°å®‰å…¨æ¼æ´", encoding="utf-8")
        print(f"[!] å‘ç°æ¼æ´ï¼Œå·²æ ‡è®°: {vuln_marker}")
    
    print(f"[âœ“] æ‰«æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {simplified_folder}")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸ç®€åŒ–è¾“å‡ºä¸åŒï¼‰
    if str(report_folder.resolve()) != str(simplified_folder.resolve()):
        try:
            shutil.rmtree(report_folder)
            print(f"[âœ“] æ¸…ç†ä¸´æ—¶ç›®å½•: {report_folder}")
        except Exception as e:
            print(f"[!] æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")


def save_non_200_urls_by_domain(non_200_urls_all, url_root_map):
    # å¤„ç†ç‰¹æ®ŠçŠ¶æ€ç ï¼š403, 404ç­‰
    status_folders = [403, 404, 500, 502, 503]  # æ‰©å±•å…³æ³¨çš„çŠ¶æ€ç 
    # æŒ‰åŸŸåå’ŒçŠ¶æ€ç åˆ†ç»„ï¼š {domain: {status_code: [urls]}}
    domain_status_urls = defaultdict(lambda: defaultdict(list))

    for url, status_code in non_200_urls_all:
        if status_code in status_folders:
            root_domain = url_root_map.get(url)
            if root_domain:
                domain_status_urls[root_domain][status_code].append(url)

    # å†™å…¥æ–‡ä»¶ï¼ŒæŒ‰çŠ¶æ€ç åˆ†åˆ«ä¿å­˜åˆ°inputç›®å½•
    for domain, status_dict in domain_status_urls.items():
        domain_folder = Path("output") / domain
        input_folder = domain_folder / "input"
        input_folder.mkdir(parents=True, exist_ok=True)
        for status_code, urls in status_dict.items():
            file_path = input_folder / f"{status_code}_urls.txt"  # åŠ¨æ€æ–‡ä»¶å
            with open(file_path, "w", encoding="utf-8") as f:  # æ”¹ä¸ºwæ¨¡å¼é¿å…é‡å¤
                f.write(f"# {status_code}çŠ¶æ€ç URLåˆ—è¡¨ - {domain}\n")
                f.write(f"# æ€»è®¡: {len(urls)} ä¸ªURL\n\n")
                for u in urls:
                    f.write(u + "\n")


# ------------------------------------
# ä¸»ç¨‹åºå…¥å£
# ------------------------------------
# ä¸»ç¨‹åºå…¥å£
def generate_mock_data(target_domain):
    """ç”Ÿæˆæ¨¡æ‹ŸJSONæ•°æ®ç”¨äºæµ‹è¯•"""
    if not target_domain:
        target_domain = "example.com"
    
    print(f"[*] ä¸ºç›®æ ‡åŸŸå {target_domain} ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
    
    # ç¡®ä¿tempç›®å½•å­˜åœ¨
    os.makedirs("temp", exist_ok=True)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    mock_data = []
    
    # ä¸»åŸŸå
    mock_data.append({
        "url": f"https://{target_domain}",
        "location": f"https://{target_domain}",
        "title": f"{target_domain.split('.')[0].title()} Official Website",
        "status_code": 200,
        "content_length": 12345,
        "body": f"<html><head><title>{target_domain}</title></head><body>Welcome to {target_domain}</body></html>",
        "cert": {"subject": {"common_name": target_domain}},
        "favicon": {"mmh3": "123456789"},
        "a": ["192.168.1.100"],
        "cname": []
    })
    
    # wwwå­åŸŸå
    mock_data.append({
        "url": f"https://www.{target_domain}",
        "location": f"https://www.{target_domain}",
        "title": f"www.{target_domain}",
        "status_code": 200,
        "content_length": 11234,
        "body": f"<html><head><title>www.{target_domain}</title></head><body>Main website</body></html>",
        "cert": {"subject": {"common_name": f"*.{target_domain}"}},
        "favicon": {"mmh3": "987654321"},
        "a": ["192.168.1.101"],
        "cname": []
    })
    
    # APIå­åŸŸå
    mock_data.append({
        "url": f"https://api.{target_domain}",
        "location": f"https://api.{target_domain}",
        "title": "API Gateway",
        "status_code": 200,
        "content_length": 567,
        "body": '{"status":"ok","version":"1.0"}',
        "cert": {"subject": {"common_name": f"api.{target_domain}"}},
        "favicon": {"mmh3": "555666777"},
        "a": ["192.168.1.102"],
        "cname": []
    })
    
    # ç®¡ç†é¢æ¿
    mock_data.append({
        "url": f"https://admin.{target_domain}",
        "location": f"https://admin.{target_domain}/login",
        "title": "Admin Panel - Login Required",
        "status_code": 302,
        "content_length": 1234,
        "body": "<html><head><title>Admin Login</title></head><body>Please login</body></html>",
        "cert": {"subject": {"common_name": f"admin.{target_domain}"}},
        "favicon": {"mmh3": "111222333"},
        "a": ["192.168.1.103"],
        "cname": []
    })
    
    # é‚®ä»¶æœåŠ¡å™¨
    mock_data.append({
        "url": f"https://mail.{target_domain}",
        "location": f"https://mail.{target_domain}",
        "title": "Webmail Login",
        "status_code": 200,
        "content_length": 5678,
        "body": "<html><head><title>Webmail</title></head><body>Mail server</body></html>",
        "cert": {"subject": {"common_name": f"mail.{target_domain}"}},
        "favicon": {"mmh3": "444555666"},
        "a": ["192.168.1.104"],
        "cname": []
    })
    
    # å†™å…¥æ¨¡æ‹Ÿæ•°æ®åˆ°JSONæ–‡ä»¶
    with open(RESULT_JSON_PATH, "w", encoding="utf-8") as f:
        for item in mock_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print(f"[âœ“] å·²ç”Ÿæˆ {len(mock_data)} æ¡æ¨¡æ‹Ÿæ•°æ®åˆ° {RESULT_JSON_PATH}")
    print(f"[*] åŒ…å«åŸŸå: {[item['url'] for item in mock_data]}")

def main():
    init_dirs()
    filter_domains = load_filter_domains(FILTER_DOMAIN_PATH)
    cdn_ranges = load_cdn_ranges(CDN_LIST_PATH)
    existing_cdn_dyn_ips = {line.strip() for line in open(CDN_DYNAMIC_PATH, encoding="utf-8")} if os.path.exists(CDN_DYNAMIC_PATH) else set()

    # è¯»å–ç›®æ ‡åŸŸå
    target_domain = None
    target_file_path = "data/input/url"
    if os.path.exists(target_file_path):
        with open(target_file_path, "r", encoding="utf-8") as f:
            target_domain = f.read().strip()
        print(f"[*] æ£€æµ‹åˆ°ç›®æ ‡åŸŸå: {target_domain}")

    if not os.path.exists(RESULT_JSON_PATH):
        if '-test' in sys.argv:
            print("[*] æµ‹è¯•æ¨¡å¼ï¼šç”Ÿæˆæ¨¡æ‹ŸJSONæ•°æ®")
            generate_mock_data(target_domain)
        else:
            print("[X] ç»“æœæ–‡ä»¶ä¸å­˜åœ¨")
            return

    with open(RESULT_JSON_PATH, "r", encoding="utf-8") as f:
        lines = f.readlines()

    print("[*] å¼€å§‹å¤šè¿›ç¨‹è§£æ JSON è®°å½•...")
    cpu_count = min(multiprocessing.cpu_count(), 8)  # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°
    chunk_size = max(500, len(lines) // (cpu_count * 2))  # åŠ¨æ€è°ƒæ•´chunkå¤§å°
    chunks = list(chunked_iterable(lines, chunk_size))
    
    print(f"[*] ä½¿ç”¨ {cpu_count} ä¸ªè¿›ç¨‹ï¼Œ{len(chunks)} ä¸ªchunkï¼Œæ¯ä¸ªchunkçº¦ {chunk_size} è¡Œ")

    worker = partial(parse_json_lines_chunk,
                     cdn_ranges=cdn_ranges,
                     existing_cdn_dyn_ips=existing_cdn_dyn_ips,
                     filter_domains=filter_domains,
                     target_domain=target_domain)

    pool = multiprocessing.Pool(cpu_count)

    domain_ip_map = defaultdict(set)
    url_title_list = []
    url_root_map = {}
    url_body_info_map = {}  # âœ… æ–°å¢
    non_200_urls_all = []  # æ–°å¢ï¼Œå­˜å‚¨æ‰€æœ‰é200/301/302 url
    redirect_domains_all = set()  # æ–°å¢ï¼Œå­˜å‚¨æ‰€æœ‰è·³è½¬å‘ç°çš„åŸŸå

    with tqdm(total=len(chunks), desc="å¤„ç†è®°å½•") as pbar:
        for dmap, titles, urlmap, url_body_info, non_200_urls, redirect_domains in pool.imap_unordered(worker, chunks):
            for k, v in dmap.items():
                domain_ip_map[k].update(v)
            url_title_list.extend(titles)
            url_root_map.update(urlmap)
            url_body_info_map.update(url_body_info)  # âœ… åˆå¹¶è¿‡æ»¤åæ•°æ®
            non_200_urls_all.extend(non_200_urls)
            redirect_domains_all.update(redirect_domains)  # åˆå¹¶è·³è½¬åŸŸå

            pbar.update(1)

    pool.close()
    pool.join()
    # å‡†å¤‡æŒ‰åŸŸååˆ†ç»„ urls å’Œ titles
    domain_urls_map = defaultdict(set)
    domain_titles_map = {}
    for url, root_domain in url_root_map.items():
        domain_urls_map[root_domain].add(url)

    for url, title, cert, ico, body, url_ips,ico_mmh3,bd_mmh3 in url_title_list:
        domain_titles_map[url] = (title, cert, ico, body, url_ips,ico_mmh3,bd_mmh3)

    #403
    save_non_200_urls_by_domain(non_200_urls_all, url_root_map)

    # è¾“å‡ºè¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*50}")
    print(f"ğŸ“Š åŸŸåè¿‡æ»¤ç»Ÿè®¡:")
    print(f"{'='*50}")
    print(f"ğŸ” å‘ç°è·³è½¬åŸŸå: {len(redirect_domains_all)} ä¸ª")
    if len(redirect_domains_all) > 0:
        print(f"   è·³è½¬åŸŸåç¤ºä¾‹: {list(redirect_domains_all)[:5]}")
    print(f"{'='*50}")

    
    # å¼‚æ­¥ä»»åŠ¡æ”¾åˆ° asyncio.run ä¸­æ‰§è¡Œ
    asyncio.run(run_domain_tasks(domain_ip_map, domain_urls_map, domain_titles_map, cdn_ranges, filter_domains, existing_cdn_dyn_ips, url_body_info_map, redirect_domains_all))


async def run_domain_tasks(domain_ip_map, domain_urls_map, domain_titles_map, cdn_ranges, filter_domains, existing_cdn_dyn_ips, url_body_info_map, redirect_domains=None):
    global SKIP_CURRENT_DOMAIN
    print("[*] å¼€å§‹é€ä¸ªæ‰§è¡ŒåŸŸåæµç¨‹...")
    sorted_domains = sorted(domain_urls_map.keys(), key=natural_sort_key)

    for domain in sorted_domains:
        if SKIP_CURRENT_DOMAIN:
            print(f"[!] è·³è¿‡åŸŸå: {domain}")
            SKIP_CURRENT_DOMAIN = False
            continue

        try:
            ips = domain_ip_map[domain]
            urls = sorted(domain_urls_map.get(domain, []))
            titles = {u: domain_titles_map.get(u, ("", "", "", "", (), "", "")) for u in urls}
            await per_domain_flow_sync_async(domain, ips, urls, titles, cdn_ranges, filter_domains, existing_cdn_dyn_ips, url_body_info_map, redirect_domains)
        except asyncio.CancelledError:
            print(f"[!] å½“å‰ä»»åŠ¡è¢«å–æ¶ˆ: {domain}")
            continue
        except Exception as e:
            print(f"[!] æ‰§è¡Œ {domain} å‡ºé”™: {e}")


# ------------------------------------
if __name__ == "__main__":
    signal.signal(signal.SIGINT, handle_sigint)   # Ctrl+C
    signal.signal(signal.SIGQUIT, handle_sigquit) # Ctrl+\
    main()
